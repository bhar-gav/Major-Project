{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text files in /home/bhargav/Desktop/model/main-mds/Multi-doc-summarizaton/datasets/final/Whole_text_data: 608\n",
      "Total number of text files in /home/bhargav/Desktop/model/main-mds/Multi-doc-summarizaton/datasets/final/Summaries: 608\n",
      "Total number of text files in /home/bhargav/Desktop/model/main-mds/Multi-doc-summarizaton/datasets/final/Summary_Data_new: 608\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "relative_paths = [\n",
    "    '../../datasets/final/Whole_text_data',\n",
    "    '../../datasets/final/Summaries',\n",
    "    '../../datasets/final/Summary_Data_new'\n",
    "]\n",
    "\n",
    "# Function to count text files in a directory\n",
    "def count_text_files(directory):\n",
    "    if not os.path.isdir(directory):\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return 0\n",
    "    else:\n",
    "        files = os.listdir(directory)\n",
    "        text_files = [f for f in files if f.endswith('.txt')]\n",
    "        return len(text_files)\n",
    "\n",
    "# Process each folder\n",
    "for relative_path in relative_paths:\n",
    "    directory = os.path.abspath(relative_path)\n",
    "    num_text_files = count_text_files(directory)\n",
    "    print(f\"Total number of text files in {directory}: {num_text_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "news_category=[\"task\"]\n",
    "\n",
    "row_doc = os.path.abspath(relative_paths[0])  # assuming data is good quality\n",
    "summary_doc = os.path.abspath(relative_paths[1]) \n",
    "\n",
    "data={\"articles\":[], \"summaries\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                        row_article  \\\n",
      "0  P95-1021  \\nD-Tree Grammars\\n\\ndesigned to share some of...   \n",
      "1  P05-1073  \\nJoint Learning Improves Semantic Role Labeli...   \n",
      "2  D09-1127  \\nBilingually-Constrained (Monolingual) Shift-...   \n",
      "3  P02-1017  \\nA Generative Constituent-Context Model For I...   \n",
      "4  J90-1003  \\nWord Association Norms Mutual Information An...   \n",
      "\n",
      "                                             summary  \n",
      "0  We define a new grammar formalism, called D-Tr...  \n",
      "1  Title: Joint Learning Improves Semantic Role L...  \n",
      "2  Conclusion: We have presented a novel parsing ...  \n",
      "3  Title: A Generative Constituent-Context Model ...  \n",
      "4  We began this paper with the psycholinguistic ...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "relative_paths = [\n",
    "    '../../datasets/final/Whole_text_data',\n",
    "    '../../datasets/final/Summaries',\n",
    "    '../../datasets/final/Summary_Data_new'\n",
    "]\n",
    "\n",
    "\n",
    "# Define directories containing the articles and summaries\n",
    "articles_dir = os.path.abspath(relative_paths[0]) \n",
    "summaries_dir = os.path.abspath(relative_paths[1]) \n",
    "\n",
    "# Initialize lists to store filenames and their content\n",
    "article_ids = []\n",
    "article_texts = []\n",
    "summary_ids = []\n",
    "summary_texts = []\n",
    "\n",
    "# Function to extract the first 8 digits from filenames\n",
    "def extract_id(filename):\n",
    "    return filename[:8]\n",
    "\n",
    "# Read articles\n",
    "for filename in os.listdir(articles_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        path = os.path.join(articles_dir, filename)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            article_ids.append(extract_id(filename))\n",
    "            article_texts.append(file.read())\n",
    "\n",
    "# Read summaries\n",
    "for filename in os.listdir(summaries_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        path = os.path.join(summaries_dir, filename)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            summary_ids.append(extract_id(filename))\n",
    "            summary_texts.append(file.read())\n",
    "\n",
    "# Create DataFrames\n",
    "articles_df = pd.DataFrame({\n",
    "    'id': article_ids,\n",
    "    'row_article': article_texts\n",
    "})\n",
    "\n",
    "summaries_df = pd.DataFrame({\n",
    "    'id': summary_ids,\n",
    "    'summary': summary_texts\n",
    "})\n",
    "\n",
    "# Merge the DataFrames on the 'id' column\n",
    "combined_df = pd.merge(articles_df, summaries_df, on='id', how='inner')\n",
    "\n",
    "# Display the first few rows\n",
    "print(combined_df.head())\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                        row_article  \\\n",
      "0  P95-1021  d tree grammars designed to share some of the ...   \n",
      "1  P05-1073  joint learning improves semantic role labeling...   \n",
      "2  D09-1127  bilingually constrained monolingual shift redu...   \n",
      "3  P02-1017  a generative constituent context model for imp...   \n",
      "4  J90-1003  word association norms mutual information and ...   \n",
      "\n",
      "                                             summary  \n",
      "0  we define a new grammar formalism called d tre...  \n",
      "1  title joint learning improves semantic role la...  \n",
      "2  conclusion we have presented a novel parsing p...  \n",
      "3  title a generative constituent context model f...  \n",
      "4  we began this paper with the psycholinguistic ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Normalize whitespace (replace multiple spaces with a single space)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand common contractions\n",
    "    contractions = {\n",
    "        \"what's\": \"what is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"must've\": \"must have\"\n",
    "    }\n",
    "    \n",
    "    for contraction, expanded in contractions.items():\n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expanded, text)\n",
    "\n",
    "    # Replace special characters with a space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces that might have been introduced\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to the 'row_article' and 'summary' columns\n",
    "combined_df['row_article'] = combined_df['row_article'].apply(clean_text)\n",
    "combined_df['summary'] = combined_df['summary'].apply(clean_text)\n",
    "\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in combined_df.columns:\n",
    "#     combined_df[col] = combined_df[col].apply(lambda x: cleantext(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                        row_article  \\\n",
      "0  P95-1021  d tree grammars designed to share some of the ...   \n",
      "1  P05-1073  joint learning improves semantic role labeling...   \n",
      "2  D09-1127  bilingually constrained monolingual shift redu...   \n",
      "3  P02-1017  a generative constituent context model for imp...   \n",
      "4  J90-1003  word association norms mutual information and ...   \n",
      "\n",
      "                                             summary  \n",
      "0  we define a new grammar formalism called d tre...  \n",
      "1  title joint learning improves semantic role la...  \n",
      "2  conclusion we have presented a novel parsing p...  \n",
      "3  title a generative constituent context model f...  \n",
      "4  we began this paper with the psycholinguistic ...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "print(combined_df.head())\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_article_length  summary_length\n",
      "count          608.000000       608.00000\n",
      "mean         24376.133224       781.37500\n",
      "std           7824.453872       256.84481\n",
      "min           5035.000000       338.00000\n",
      "25%          20711.500000       626.00000\n",
      "50%          24153.500000       736.00000\n",
      "75%          27471.500000       901.00000\n",
      "max         115780.000000      4022.00000\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics about the text lengths\n",
    "combined_df['row_article_length'] = combined_df['row_article'].apply(len)\n",
    "combined_df['summary_length'] = combined_df['summary'].apply(len)\n",
    "\n",
    "print(combined_df[['row_article_length', 'summary_length']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicate IDs found\n",
      "608 unique IDs\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate IDs\n",
    "print(combined_df['id'].duplicated().sum(), \"duplicate IDs found\")\n",
    "\n",
    "# Check for unique IDs\n",
    "print(len(combined_df['id'].unique()), \"unique IDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /home/bhargav/Desktop/model/main-mds/Multi-doc-summarizaton/model/notebook/cleaned_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add an index column\n",
    "combined_df.insert(0, 'index', range(len(combined_df)))\n",
    "# Define the file path for the CSV output\n",
    "output_csv_path = os.path.abspath('cleaned_articles.csv')\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file with UTF-8 encoding\n",
    "combined_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Tokenization\n",
    "'''\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "      <th>row_article_length</th>\n",
       "      <th>summary_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>P95-1021</td>\n",
       "      <td>d tree grammars designed to share some of the ...</td>\n",
       "      <td>we define a new grammar formalism called d tre...</td>\n",
       "      <td>40760</td>\n",
       "      <td>953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P05-1073</td>\n",
       "      <td>joint learning improves semantic role labeling...</td>\n",
       "      <td>title joint learning improves semantic role la...</td>\n",
       "      <td>24452</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>D09-1127</td>\n",
       "      <td>bilingually constrained monolingual shift redu...</td>\n",
       "      <td>conclusion we have presented a novel parsing p...</td>\n",
       "      <td>24503</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>P02-1017</td>\n",
       "      <td>a generative constituent context model for imp...</td>\n",
       "      <td>title a generative constituent context model f...</td>\n",
       "      <td>24502</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>J90-1003</td>\n",
       "      <td>word association norms mutual information and ...</td>\n",
       "      <td>we began this paper with the psycholinguistic ...</td>\n",
       "      <td>34693</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        id                                        row_article  \\\n",
       "0      0  P95-1021  d tree grammars designed to share some of the ...   \n",
       "1      1  P05-1073  joint learning improves semantic role labeling...   \n",
       "2      2  D09-1127  bilingually constrained monolingual shift redu...   \n",
       "3      3  P02-1017  a generative constituent context model for imp...   \n",
       "4      4  J90-1003  word association norms mutual information and ...   \n",
       "\n",
       "                                             summary  row_article_length  \\\n",
       "0  we define a new grammar formalism called d tre...               40760   \n",
       "1  title joint learning improves semantic role la...               24452   \n",
       "2  conclusion we have presented a novel parsing p...               24503   \n",
       "3  title a generative constituent context model f...               24502   \n",
       "4  we began this paper with the psycholinguistic ...               34693   \n",
       "\n",
       "   summary_length  \n",
       "0             953  \n",
       "1             583  \n",
       "2             776  \n",
       "3             677  \n",
       "4             852  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_articles.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list(df.row_article)\n",
    "summaries = list(df.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhargav/anaconda3/envs/pytorch_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30522\n",
      "Sample Sequences: [tensor([  101,  1040,  3392,  8035,  2015,  2881,  2000,  3745,  2070,  1997,\n",
      "         1996, 12637,  1997,  6415,  2096, 27363,  2070,  2049, 12546,  2048,\n",
      "         5512,  3136,  2170,  4942,  8043,  3508,  1998,  2905, 20621,  3258,\n",
      "         1996,  2087,  8200, 10768,  7113,  2546,  2008,  4406,  6415,  2045,\n",
      "         2003,  3143,  6375,  3012,  1999,  1996,  2126,  2008,  1996, 14396,\n",
      "        16105,  9289,  5167,  4942,  8043,  3508,  2467, 14788,  2000, 13711,\n",
      "         3370,  1998,  2905, 20621,  3258,  2000, 16913,  2072,  7297,  4406,\n",
      "         6415,  3073,  1037,  6375,  4106,  2005,  1059, 14227, 21818,  3672,\n",
      "         1999,  2394,  1998, 13329,  2072,  2750,  1996,  2755,  2008,  1996,\n",
      "         1059,  2232,  5783,  1999, 13329,  2072,  3544,  1999,  6251,  2117,\n",
      "         2597,  1998,  2025,  6251,  3988,  2597,  2004,  1999,  2394,  2057,\n",
      "         9375,  1037,  2047,  8035,  5337,  2964,  2170,  1040,  3392, 18653,\n",
      "         2013,  2147,  2006,  3392, 13562,  8035,  2015,  6415, 26645,  3802,\n",
      "         2632,  3339,  1037, 16183, 11638,  3444,  1997,  6415,  2003,  1996,\n",
      "         3668,  5884,  1997, 10246,  2009,  3640,  2169,  4732,  3252,  2064,\n",
      "         2022,  3378,  2007,  1037, 16105,  9289,  8875,  2004,  1999, 16105,\n",
      "         9289,  3550,  8318,  8490, 26645,  2889,  5144,  3141,  2000,  1996,\n",
      "        16105,  9289,  8875,  2107,  2004,  4942, 16280, 20255,  3989,  3820,\n",
      "         3056,  4127,  1997,  2773,  2344,  8386,  2064,  2022,  5228,  2306,\n",
      "         1996,  4732,  2358,  6820,  2278,  1047,  3217,  2818,  3055,  3581,\n",
      "         2826,  1999,  2804,  3961, 12859,  3085,  2664,  2037, 11416,  6024,\n",
      "         3977,  2003,  7182,  2000,  4070,  2005,  3056, 19962,  2696, 13306,\n",
      "        13352,  2008,  2009,  2038,  2042,  5275,  4682,  3458,  6123,  2489,\n",
      "         8035,  2015, 12935,  2290, 11895, 22669,  3106,  6415,  2174,  2038,\n",
      "         2048, 12546,  2029,  3073,  1996, 14354,  2005,  2023,  1996,  2034,\n",
      "         3291,  6936,  1999,  2930,  2008,  1996,  1997, 20885,  1998, 20621,\n",
      "         3258,  2079,  2025,  4949,  4550,  2135,  3031,  1996,  4262,  1997,\n",
      "        13711,  3370,  1998, 14080,  1037,  2117,  3291,  6936,  1999,  2930,\n",
      "         1015,  1016,  2038,  2000,  2079,  2007,  1996,  1997,  3073, 16478,\n",
      "         2005,  3056, 19962,  2696, 13306, 13352,  1999,  4975, 26718,  2290,\n",
      "         2057,  2031,  2699,  2000,  9462,  2122,  3471,  2096,  3588,  4752,\n",
      "         3406,  2054,  2057,  2156,  2004,  1996,  3145, 12637,  1997,  3327,\n",
      "         2049, 11792,  5884,  1997, 10246,  1999,  2930,  1015,  1017,  2057,\n",
      "         8970,  2070,  1997,  1996,  3145,  2838,  1997,  4863,  2129,  2027,\n",
      "         2024,  3832,  2000,  4769,  3471,  2008,  2057,  2031,  4453,  2007,\n",
      "         1015,  1015, 29280,  2015,  1998, 12530, 15266,  3136,  1997, 20885,\n",
      "         1998, 20621,  3258, 14396,  2048, 16105,  9289,  5167,  2009,  2003,\n",
      "         3568,  3019,  2000, 17841,  2122,  3136,  2004,  7411,  1037,  3622,\n",
      "        12158,  7189,  2090,  1996,  2048, 16105,  9289,  5167,  8419,  1037,\n",
      "         7189,  1997, 13711,  3370,  3653, 16467,  2906, 22850,  4765,  7189,\n",
      "         2030,  1997, 14080,  1999, 11850, 12935,  2290,  2241,  8107,  2122,\n",
      "         4262,  2024,  2069, 24655,  2174,  2027,  5050,  2590, 12158, 26406,\n",
      "         2027,  3073,  1037,  6375,  8278,  2000, 28081,  1998,  2027,  2024,\n",
      "         2004,  8040, 25459,  2229, 11895, 22669,  2807,  7475,  2590,  1999,\n",
      "         2344,  2000,  2490,  7778, 11709,  1999,  2358, 11663, 20875,  7705,\n",
      "         2015,  1998,  6413, 14679,  1999,  2116,  7705,  2015, 13711,  3370,\n",
      "         1998, 14080,  2024,  1999,  2755,  2081, 22990,  3196,  3640,  1037,\n",
      "         3584,  8360,  1042,  3252,  1998, 24394,  8035,  2015,  2156,  1041,\n",
      "         1043, 11463,  7327,  2243,  2997,  2224,  2122, 21951,  2004,  1996,\n",
      "         4054,  3978,  2005, 19962,  2696, 13306,  6630,  2057,  2097,  3582,\n",
      "         1996, 24394,  3906,  1999,  7727,  2000, 13711,  3370,  1998, 14080,\n",
      "         2004,   102]), tensor([  101,  4101,  4083, 24840, 21641,  2535, 28847,  2750,  2172,  3522,\n",
      "         5082,  2006,  8321, 21641,  2535, 28847,  3025,  2147,  2038,  4321,\n",
      "         2109,  2981,  2465, 28295,  4298,  4117,  2007,  3584,  3830,  5537,\n",
      "         4275,  3081,  6819,  3334,  5638, 21933,  4667,  2023,  4832,  1999,\n",
      "         9762,  5688,  2000,  1996, 12158,  8089,  2008,  1037,  4563,  6685,\n",
      "         4853,  2003,  2007,  2844, 12530, 15266,  2090,  9918,  2057,  2265,\n",
      "         2129,  2000,  3857,  1037,  4101,  2944,  1997,  6685, 11048, 13543,\n",
      "         3117,  2838,  2008,  2944,  2122, 10266,  2046,  5860, 20026,  3981,\n",
      "         6024,  8833,  4179,  2906,  4275,  2023,  2291,  6162,  2015,  2019,\n",
      "         7312,  1997,  2035,  9918,  4563,  9918,  2058,  1037,  2110, 11253,\n",
      "         1996,  2396,  2981,  2465, 18095,  2005,  2751, 21515,  4232, 11968,\n",
      "         3366,  3628,  2006, 17678,  9299,  1996,  2713,  1997, 21641,  3973,\n",
      "         5754, 17287,  3064, 13058,  6525,  2107,  2004,  4853,  7159,  6243,\n",
      "         3802,  2632,  2687,  1998, 17678,  9299,  8809,  3802,  2632,  2494,\n",
      "         2038,  2081,  2009,  2825,  2000,  4503,  2152, 10640,  7778,  4275,\n",
      "         2005, 12978, 21641,  2535, 28847, 13097,  3207,  2050,  1998, 18414,\n",
      "        27528,  5874,  2526, 10975,  4215,  4819,  3802,  2632,  2432, 15990,\n",
      "         2063,  1998,  8809,  2432,  2107,  3001,  2031,  4453,  2195, 12158,\n",
      "         3973, 12774,  2838,  2005,  5860, 20026, 19185,  9918,  1998,  2037,\n",
      "        10873,  2156,  2795,  1015,  2122,  2838,  2788,  2839,  4697,  5919,\n",
      "         1997,  3265,  9918,  1998,  1996,  3653, 16467,  2009,  2003, 10358,\n",
      "         2008,  1996, 10873,  1998,  1996,  2838,  1997,  9918,  2024,  3811,\n",
      "        23900,  2005,  2742,  2045,  2024,  2524, 14679,  2008,  9918,  3685,\n",
      "        17702,  2007,  2169,  2060,  2030,  1996,  3653, 16467,  1998,  2036,\n",
      "         3730, 14679,  2005,  2742,  2003,  2009,  9832,  2008,  1037,  3653,\n",
      "        16467,  2097,  2031,  2048,  2030,  2062,  4005,  9918,  2030,  2008,\n",
      "         1037,  3653, 16467,  2109,  1999,  1996,  3161,  2376,  2097,  2031,\n",
      "         1037,  4323,  6685,  3188,  2000,  2019,  4005,  6685,  2195,  3001,\n",
      "         2031,  5100,  2107, 12530, 15266,  2005,  2742, 13097,  3207,  2050,\n",
      "         1998, 18414, 27528,  5874,  2526, 10975,  4215,  4819,  3802,  2632,\n",
      "         2432,  5953,  3802,  2632,  2494,  1998,  2195,  3001,  7864,  1999,\n",
      "         1996,  9530,  3363,  2432,  4207,  4708, 12385, 24140,  1998,  1049,\n",
      "        12098,  4226,  2480,  2432,  2174,  2057,  2265,  2008,  2045,  2024,\n",
      "         3618, 12154,  2000,  2022,  2018,  2011, 11643,  4101,  2592,  2055,\n",
      "         1037, 12034,  1055,  6685,  3252,  2057, 16599,  1037,  5860, 20026,\n",
      "         3981,  6024,  8833,  7399,  4101,  2944,  2005, 21641,  2535, 28847,\n",
      "         2029, 12374,  2062,  3795,  2838,  1998,  6162,  2015,  6020,  2836,\n",
      "         1999,  7831,  2000,  2110,  1997,  1996,  2396,  4275,  2000,  3066,\n",
      "         2007,  1996, 15078, 11619,  1997,  1996,  4708,  2057, 12666,  8790,\n",
      "         4730,  1998,  2128, 26763,  2075,  8107,  2057,  2556,  2836,  3463,\n",
      "         2006,  1996,  2337,  2432,  2544,  1997, 17678,  9299,  2006,  2751,\n",
      "         3115, 11968,  3366,  3628,  2004,  2092,  2004,  3463,  2006,  6882,\n",
      "        11968,  8583,  7013,  2011, 25869,  6200,  2243,  1055, 11968,  8043,\n",
      "        25869,  6200,  2243,  2456,  5136,  1996,  3940,  1997, 11746,  2750,\n",
      "         1996,  2367, 19962,  2696, 13306,  4460,  1997,  1996, 12599, 15672,\n",
      "         2057,  6807,  2008,  2169,  3248,  1996,  2168,  2535,  5393,  2011,\n",
      "         1996,  3830,  1999,  1996,  3574,  1997,  2023,  3168,  1997,  1996,\n",
      "        12034,  2507,  2057,  2655,  2107, 15672,  6039,  2545,  1997, 21641,\n",
      "         4395,  1998,  2256,  4708,  2003,  2445,  1037,  6251,  1998,  1037,\n",
      "         4539, 12034,  2000,  2709,  2035,  2107, 15672,  2247,  2007,  2037,\n",
      "         6149, 10873,  3568,  2028,  4942, 10230,  2243,  2003,  2000,  2177,\n",
      "         1996,   102])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer (you can choose other tokenizers as well)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode articles\n",
    "encoded_articles = [tokenizer.encode(article, add_special_tokens=True, truncation=True, padding='max_length', max_length=512) for article in articles]\n",
    "\n",
    "# Convert to tensors\n",
    "article_sequences = [torch.tensor(seq, dtype=torch.long) for seq in encoded_articles]\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "# Print vocabulary size and some sample sequences\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Sample Sequences: {article_sequences[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1040,  3392,  8035,  2015,  2881,  2000,  3745,  2070,  1997,\n",
      "         1996, 12637,  1997,  6415,  2096, 27363,  2070,  2049, 12546,  2048])\n",
      "tensor([  101,  4101,  4083, 24840, 21641,  2535, 28847,  2750,  2172,  3522,\n",
      "         5082,  2006,  8321, 21641,  2535, 28847,  3025,  2147,  2038,  4321])\n",
      "tensor([  101, 17636,  2135, 27570, 18847,  2989,  8787,  5670,  5547, 11968,\n",
      "         7741, 10776, 11968,  7741,  2048,  4155,  2038,  2042,  3491,  2000])\n"
     ]
    }
   ],
   "source": [
    "print(article_sequences[0][:20])\n",
    "print(article_sequences[1][:20])\n",
    "print(article_sequences[2][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Empirical Testing:\\n\\n  Test various sizes (e.g., 15,000, 20,000, 25,000) \\n  to find what works best for your specific task. \\n  Monitor the impact on model performance and training efficiency.\\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabraly: article and summary 15000 words (original in above settings Vocabulary Size: 30522)\n",
    "'''\n",
    "    Empirical Testing:\n",
    "\n",
    "  Test various sizes (e.g., 15,000, 20,000, 25,000) \n",
    "  to find what works best for your specific task. \n",
    "  Monitor the impact on model performance and training efficiency.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 17207\n",
      "Most common tokens: [('the', 172391), ('of', 84080), ('a', 60083), ('and', 59357), ('to', 55666), ('in', 55213), ('is', 40754), ('for', 35571), ('we', 31195), ('that', 27712), ('##s', 24310), ('as', 21446), ('are', 20165), ('this', 18449), ('with', 17913), ('on', 16853), ('by', 15721), ('be', 14775), ('from', 12367), ('word', 12029)]\n",
      "Custom Vocabulary Size: 15001\n",
      "Sample article sequences: [tensor([  186,    69,   162,  ..., 12700,  7528,   228]), tensor([ 712,  153, 1319,  ..., 4804, 2227,  992])]\n",
      "Sample summary sequences: [tensor([   9,  609,    3,  175,  162,  847, 1757,  865,  186,   69, 3753,   19,\n",
      "         134,   16,   69, 3471,  162,   11,   88, 3644,   46,   45, 4920,    3,\n",
      "        1771, 2183,   95,    2,   88,    7,    1, 1199,  235,    2, 4745,   23,\n",
      "         742,  149,  125,  722,  391,  409, 1712,  125,  531,    3, 1604, 3191,\n",
      "           5, 1261,    4,  125,   13,   12,  428, 2859,  342, 2823, 4285,  849,\n",
      "        3057,  391,    6,  155,    5,  890,  239,  347,    6,  631, 1685, 1705,\n",
      "         583,   11,    4,  911,  344,    6,  172,  583,   11, 1346,  262,    4,\n",
      "        2494,   13,    6,  503,  561, 4006, 4255,  742,    3,  983, 2479,   97,\n",
      "         198,    4,  178,  162,   11,  254,   39,  119, 2555, 4286,  199, 1929,\n",
      "          56,   49, 4635,   12,    1, 4747, 1435,    8,  142,  150,  165,  469,\n",
      "           1,  503,   10, 1426, 1270,    4, 1472,   13,   40,    6,    3,  409,\n",
      "         484, 4981, 2235, 2078, 5312, 1679,  684,   10,  240,   88,  476,  298,\n",
      "         203,   28,  531,    3,  411,  469,    2,    1,  424,  504,   73,    1,\n",
      "          32,    2,    1,   53,   57,   39,    2,    1,  238,  501,  371,    4,\n",
      "        2494,  198]), tensor([1282,  712,  153, 1319,  130,  689,  941, 1420,  949,  560, 1801,  333,\n",
      "         934, 2809,   16,  874,  130,  689,  941,  269,  134,   74, 2826,   40,\n",
      "         784,   89,  485, 2028,  778,   15,  983,  387,  286,   79, 1136, 1068,\n",
      "         412,  304,  358,  567,    9,  266,  225,    5, 1104,    3,  712,   25,\n",
      "           2,  371, 1574, 2166, 1066,   50,   10,   25,   49, 2932,  122,  586,\n",
      "         720,  832,  355,  398, 1602,  899,   79, 1782, 5182,  409, 1712,    4,\n",
      "           6,  855,   15,  431,  134,    9,   42,  234,   10,  126,   13, 1715,\n",
      "        1797,    5,   18,  614,   17, 2529,  943,    1,  371, 1574,    2,  435])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Sample data\n",
    "# articles = [\"Your list of article texts here\"]\n",
    "# summaries = [\"Your list of summary texts here\"]\n",
    "\n",
    "# Tokenize and get tokens for all texts\n",
    "def tokenize_texts(texts, tokenizer):\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        tokens.extend(tokenizer.tokenize(text))\n",
    "    return tokens\n",
    "\n",
    "# Get tokens from articles and summaries\n",
    "article_tokens = tokenize_texts(articles, tokenizer)\n",
    "summary_tokens = tokenize_texts(summaries, tokenizer)\n",
    "\n",
    "# Combine tokens and count frequencies\n",
    "all_tokens = article_tokens + summary_tokens\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Print token counts and check for unexpected results\n",
    "print(f\"Total unique tokens: {len(token_counts)}\")\n",
    "print(f\"Most common tokens: {token_counts.most_common(20)}\")\n",
    "\n",
    "# Get the most common tokens up to VOCAB_SIZE\n",
    "VOCAB_SIZE = 14999\n",
    "common_tokens = [token for token, _ in token_counts.most_common(VOCAB_SIZE)]\n",
    "\n",
    "# Build a custom vocabulary\n",
    "custom_vocab = {token: idx + 1 for idx, token in enumerate(common_tokens)}\n",
    "custom_vocab['<unk>'] = 0  # Add unknown token with index 0\n",
    "\n",
    "# Tokenize function using custom vocabulary\n",
    "def text_to_custom_sequence(text, custom_vocab, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [custom_vocab.get(token, custom_vocab['<unk>']) for token in tokens]\n",
    "\n",
    "# Convert articles and summaries to sequences using custom vocabulary\n",
    "article_sequences = [text_to_custom_sequence(article, custom_vocab, tokenizer) for article in articles]\n",
    "summary_sequences = [text_to_custom_sequence(summary, custom_vocab, tokenizer) for summary in summaries]\n",
    "\n",
    "# Convert sequences to PyTorch tensors\n",
    "article_sequences_tensor = [torch.tensor(seq, dtype=torch.long) for seq in article_sequences]\n",
    "summary_sequences_tensor = [torch.tensor(seq, dtype=torch.long) for seq in summary_sequences]\n",
    "\n",
    "# Print results\n",
    "print(f\"Custom Vocabulary Size: {len(custom_vocab)}\")\n",
    "print(f\"Sample article sequences: {article_sequences_tensor[:2]}\")\n",
    "print(f\"Sample summary sequences: {summary_sequences_tensor[:2]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
