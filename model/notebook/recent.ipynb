{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text files in /home/bhargav/Desktop/model/main-mds/Multi-doc-summarizaton/datasets/final/Whole_text_data: 608\n",
      "Total number of text files in /home/bhargav/Desktop/model/main-mds/Multi-doc-summarizaton/datasets/final/Summaries: 608\n",
      "Total number of text files in /home/bhargav/Desktop/model/main-mds/Multi-doc-summarizaton/datasets/final/Summary_Data_new: 608\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "relative_paths = [\n",
    "    '../../datasets/final/Whole_text_data',\n",
    "    '../../datasets/final/Summaries',\n",
    "    '../../datasets/final/Summary_Data_new'\n",
    "]\n",
    "\n",
    "# Function to count text files in a directory\n",
    "def count_text_files(directory):\n",
    "    if not os.path.isdir(directory):\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return 0\n",
    "    else:\n",
    "        files = os.listdir(directory)\n",
    "        text_files = [f for f in files if f.endswith('.txt')]\n",
    "        return len(text_files)\n",
    "\n",
    "# Process each folder\n",
    "for relative_path in relative_paths:\n",
    "    directory = os.path.abspath(relative_path)\n",
    "    num_text_files = count_text_files(directory)\n",
    "    print(f\"Total number of text files in {directory}: {num_text_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "news_category=[\"task\"]\n",
    "\n",
    "row_doc = os.path.abspath(relative_paths[0])  # assuming data is good quality\n",
    "summary_doc = os.path.abspath(relative_paths[1]) \n",
    "\n",
    "data={\"articles\":[], \"summaries\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                        row_article  \\\n",
      "0  P95-1021  \\nD-Tree Grammars\\n\\ndesigned to share some of...   \n",
      "1  P05-1073  \\nJoint Learning Improves Semantic Role Labeli...   \n",
      "2  D09-1127  \\nBilingually-Constrained (Monolingual) Shift-...   \n",
      "3  P02-1017  \\nA Generative Constituent-Context Model For I...   \n",
      "4  J90-1003  \\nWord Association Norms Mutual Information An...   \n",
      "\n",
      "                                             summary  \n",
      "0  We define a new grammar formalism, called D-Tr...  \n",
      "1  Title: Joint Learning Improves Semantic Role L...  \n",
      "2  Conclusion: We have presented a novel parsing ...  \n",
      "3  Title: A Generative Constituent-Context Model ...  \n",
      "4  We began this paper with the psycholinguistic ...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "relative_paths = [\n",
    "    '../../datasets/final/Whole_text_data',\n",
    "    '../../datasets/final/Summaries',\n",
    "    '../../datasets/final/Summary_Data_new'\n",
    "]\n",
    "\n",
    "\n",
    "# Define directories containing the articles and summaries\n",
    "articles_dir = os.path.abspath(relative_paths[0]) \n",
    "summaries_dir = os.path.abspath(relative_paths[1]) \n",
    "\n",
    "# Initialize lists to store filenames and their content\n",
    "article_ids = []\n",
    "article_texts = []\n",
    "summary_ids = []\n",
    "summary_texts = []\n",
    "\n",
    "# Function to extract the first 8 digits from filenames\n",
    "def extract_id(filename):\n",
    "    return filename[:8]\n",
    "\n",
    "# Read articles\n",
    "for filename in os.listdir(articles_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        path = os.path.join(articles_dir, filename)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            article_ids.append(extract_id(filename))\n",
    "            article_texts.append(file.read())\n",
    "\n",
    "# Read summaries\n",
    "for filename in os.listdir(summaries_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        path = os.path.join(summaries_dir, filename)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            summary_ids.append(extract_id(filename))\n",
    "            summary_texts.append(file.read())\n",
    "\n",
    "# Create DataFrames\n",
    "articles_df = pd.DataFrame({\n",
    "    'id': article_ids,\n",
    "    'row_article': article_texts\n",
    "})\n",
    "\n",
    "summaries_df = pd.DataFrame({\n",
    "    'id': summary_ids,\n",
    "    'summary': summary_texts\n",
    "})\n",
    "\n",
    "# Merge the DataFrames on the 'id' column\n",
    "combined_df = pd.merge(articles_df, summaries_df, on='id', how='inner')\n",
    "\n",
    "# Display the first few rows\n",
    "print(combined_df.head())\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                        row_article  \\\n",
      "0  P95-1021  d tree grammars designed to share some of the ...   \n",
      "1  P05-1073  joint learning improves semantic role labeling...   \n",
      "2  D09-1127  bilingually constrained monolingual shift redu...   \n",
      "3  P02-1017  a generative constituent context model for imp...   \n",
      "4  J90-1003  word association norms mutual information and ...   \n",
      "\n",
      "                                             summary  \n",
      "0  we define a new grammar formalism called d tre...  \n",
      "1  title joint learning improves semantic role la...  \n",
      "2  conclusion we have presented a novel parsing p...  \n",
      "3  title a generative constituent context model f...  \n",
      "4  we began this paper with the psycholinguistic ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Normalize whitespace (replace multiple spaces with a single space)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand common contractions\n",
    "    contractions = {\n",
    "        \"what's\": \"what is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"must've\": \"must have\"\n",
    "    }\n",
    "    \n",
    "    for contraction, expanded in contractions.items():\n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expanded, text)\n",
    "\n",
    "    # Replace special characters with a space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces that might have been introduced\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to the 'row_article' and 'summary' columns\n",
    "combined_df['row_article'] = combined_df['row_article'].apply(clean_text)\n",
    "combined_df['summary'] = combined_df['summary'].apply(clean_text)\n",
    "\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in combined_df.columns:\n",
    "#     combined_df[col] = combined_df[col].apply(lambda x: cleantext(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                        row_article  \\\n",
      "0     d tree grammars designed to share some of the ...   \n",
      "1     joint learning improves semantic role labeling...   \n",
      "2     bilingually constrained monolingual shift redu...   \n",
      "3     a generative constituent context model for imp...   \n",
      "4     word association norms mutual information and ...   \n",
      "\n",
      "                                             summary  \n",
      "0  we define a new grammar formalism called d tre...  \n",
      "1  title joint learning improves semantic role la...  \n",
      "2  conclusion we have presented a novel parsing p...  \n",
      "3  title a generative constituent context model f...  \n",
      "4  we began this paper with the psycholinguistic ...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "print(combined_df.head())\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_article_length  summary_length\n",
      "count          608.000000      608.000000\n",
      "mean         19660.417763      630.314145\n",
      "std           6359.641583      215.743338\n",
      "min           3388.000000      283.000000\n",
      "25%          16814.250000      503.000000\n",
      "50%          19409.000000      600.500000\n",
      "75%          22238.250000      724.000000\n",
      "max          96426.000000     3638.000000\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics about the text lengths\n",
    "combined_df['row_article_length'] = combined_df['row_article'].apply(len)\n",
    "combined_df['summary_length'] = combined_df['summary'].apply(len)\n",
    "\n",
    "print(combined_df[['row_article_length', 'summary_length']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607 duplicate IDs found\n",
      "1 unique IDs\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate IDs\n",
    "print(combined_df['id'].duplicated().sum(), \"duplicate IDs found\")\n",
    "\n",
    "# Check for unique IDs\n",
    "print(len(combined_df['id'].unique()), \"unique IDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /home/bhargav/Desktop/model/main-mds/Multi-doc-summarizaton/model/notebook/cleaned_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add an index column\n",
    "combined_df.insert(0, 'index', range(len(combined_df)))\n",
    "# Define the file path for the CSV output\n",
    "output_csv_path = os.path.abspath('cleaned_articles.csv')\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file with UTF-8 encoding\n",
    "combined_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Tokenization\n",
    "'''\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "      <th>row_article_length</th>\n",
       "      <th>summary_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>grammars designed to share some of the advanta...</td>\n",
       "      <td>we define a new grammar called arises from wor...</td>\n",
       "      <td>32158</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joint learning improves semantic role labeling...</td>\n",
       "      <td>joint learning improves semantic role despite ...</td>\n",
       "      <td>20204</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parsing jointly parsing two languages has been...</td>\n",
       "      <td>we have presented a novel parsing monolingual ...</td>\n",
       "      <td>18732</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a generative model for improved grammar induct...</td>\n",
       "      <td>a generative model for improved grammar we pre...</td>\n",
       "      <td>19784</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>word association norms mutual information and ...</td>\n",
       "      <td>we began this paper with the psycholinguistic ...</td>\n",
       "      <td>26290</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id                                        row_article  \\\n",
       "0      0 NaN  grammars designed to share some of the advanta...   \n",
       "1      1 NaN  joint learning improves semantic role labeling...   \n",
       "2      2 NaN  parsing jointly parsing two languages has been...   \n",
       "3      3 NaN  a generative model for improved grammar induct...   \n",
       "4      4 NaN  word association norms mutual information and ...   \n",
       "\n",
       "                                             summary  row_article_length  \\\n",
       "0  we define a new grammar called arises from wor...               32158   \n",
       "1  joint learning improves semantic role despite ...               20204   \n",
       "2  we have presented a novel parsing monolingual ...               18732   \n",
       "3  a generative model for improved grammar we pre...               19784   \n",
       "4  we began this paper with the psycholinguistic ...               26290   \n",
       "\n",
       "   summary_length  \n",
       "0             737  \n",
       "1             494  \n",
       "2             586  \n",
       "3             561  \n",
       "4             700  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_articles.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list(df.row_article)\n",
    "summaries = list(df.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhargav/anaconda3/envs/pytorch_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30522\n",
      "Sample Sequences: [tensor([  101,  8035,  2015,  2881,  2000,  3745,  2070,  1997,  1996, 12637,\n",
      "         1997,  6415,  2096, 27363,  2070,  2049,  2048,  5512,  3136,  2170,\n",
      "         4942,  8043,  3508,  1998,  1996,  2087,  8200, 10768,  7113,  2546,\n",
      "         4406,  2045,  2003,  3143,  6375,  3012,  1999,  1996,  2126,  2008,\n",
      "         1996, 14396, 16105,  9289,  4942,  8043,  3508,  2467, 14788,  2000,\n",
      "        13711,  3370,  1998,  2000,  4406,  3073,  1037,  6375,  4106,  2005,\n",
      "         1059, 14227, 21818,  3672,  1999,  2394,  1998,  2750,  1996,  2755,\n",
      "         2008,  1996,  1059,  2232,  5783,  1999, 13329,  2072,  3544,  1999,\n",
      "         1998,  2025,  2597,  2004,  1999,  2057,  9375,  1037,  2047,  8035,\n",
      "         2170, 18653,  2013,  2147,  2006, 13562,  8035,  2015,  3802,  1037,\n",
      "        16183, 11638,  3444,  1997,  6415,  2003,  1996,  3668,  5884,  1997,\n",
      "        10246,  2009,  2169,  4732,  3252,  2064,  2022,  3378,  2007,  1037,\n",
      "        16105,  9289,  8875,  1999, 16105,  9289,  3550,  5144,  3141,  2000,\n",
      "         1996, 16105,  9289,  8875,  2004,  3056,  4127,  1997,  2773,  2344,\n",
      "         2064,  2022,  5228,  2306,  1996,  4732,  1999,  3961,  2664,  2037,\n",
      "        11416,  6024,  3977,  2003,  7182,  2000,  4070,  2005,  3056, 19962,\n",
      "         2696, 13306, 13352,  2009,  2038,  2042,  4682,  3458,  8035,  2015,\n",
      "         2038,  2048, 12546,  2029,  3073,  1996, 14354,  2005,  2023,  1996,\n",
      "         2034,  3291,  1999,  2930,  2008,  1996,  1997, 20885,  1998, 20621,\n",
      "         3258,  2079,  2025,  4949,  4550,  2135,  3031,  1996,  4262,  1997,\n",
      "        13711,  3370,  1998,  1037,  2117,  3291,  1999,  2930,  2038,  2000,\n",
      "         2079,  2007,  1996,  1997,  3073, 16478,  2005,  3056, 19962,  2696,\n",
      "        13306,  1999,  4975, 26718,  2290,  2057,  2031,  2699,  2000,  9462,\n",
      "         2122,  3471,  2096,  3588,  4752,  3406,  2054,  2057,  2156,  2004,\n",
      "         1996,  3145, 12637,  1997,  2049, 11792,  5884,  1997,  1999,  2930,\n",
      "         2057,  8970,  2070,  1997,  1996,  3145,  2838,  1997,  4863,  2129,\n",
      "         2027,  2024,  3832,  2000,  4769,  3471,  2008,  2057,  2031,  4453,\n",
      "         2007, 29280,  2015,  1998, 12530, 15266,  3136,  1997, 20885,  1998,\n",
      "        20621,  3258, 14396,  2048, 16105,  9289,  2009,  2003,  3568,  3019,\n",
      "         2000, 17841,  2122,  3136,  2004,  7411,  1037,  3622, 12158,  7189,\n",
      "         2090,  1996,  2048, 16105,  9289,  8419,  1037,  7189,  1997, 13711,\n",
      "         3370,  2030,  1997,  1999, 11850,  2122,  4262,  2024,  2069,  2027,\n",
      "         5050,  2590, 12158,  2027,  3073,  1037,  6375,  8278,  2000,  1998,\n",
      "         2027,  2004,  8040, 25459,  2229, 11895, 22669,  2590,  1999,  2344,\n",
      "         2000,  2490,  7778, 11709,  1999,  2358, 11663, 20875,  7705,  2015,\n",
      "         1998,  6413, 14679,  1999,  2116, 13711,  3370,  1998, 14080,  2024,\n",
      "         1999,  2755,  2081,  3640,  1037,  3584,  8360,  1998, 24394,  8035,\n",
      "         2015,  2224,  2122, 21951,  2004,  1996,  4054,  3978,  2005, 19962,\n",
      "         2696, 13306,  2057,  2097,  3582,  1996, 24394,  3906,  1999,  7727,\n",
      "         2000, 13711,  3370,  1998, 14080,  2004, 19962,  2696, 13306,  2004,\n",
      "         5159,  2011,  8223, 18912,  1998, 26645,  2005,  1996,  5197,  1997,\n",
      "         1996, 24394,  3252,  2965,  2008,  2025,  2069,  1996,  5173,  3392,\n",
      "         2003,  1997,  2021,  2036,  1996,  3136,  2011,  2029,  2057,  4663,\n",
      "         2009,  2013,  4732,  2023,  2592,  2003, 12359,  1999,  1996, 29280,\n",
      "         3392,  2004,  1996,  3136,  2024,  2025,  2109,  2096, 20885,  2003,\n",
      "         2109,  2069,  2000,  5587,  1037, 20621,  3258,  2003,  2109,  2119,\n",
      "         2005, 14080,  1998, 19118,  2389, 13711,  3370,  2071,  2025,  2022,\n",
      "         8971, 27423,  2011, 20885,  2138,  1997,  1996,  4598,  1997, 19962,\n",
      "         2696, 13306,  2107,  2004,  1999,  2045,  2003,  2019,  4297,  5644,\n",
      "        27870,  9407,  1999,  1996, 20396,  3012,  1997,  1996,  3136,  2109,\n",
      "         2005, 13711,  3370,  1999, 15087, 13711,  2015,  2024, 17316,  2046,\n",
      "         2037,   102]), tensor([  101,  4101,  4083, 24840, 21641,  2535, 28847,  2750,  2172,  3522,\n",
      "         5082,  2006,  8321, 21641,  2535,  3025,  2147,  2038,  4321,  2109,\n",
      "         2981,  4298,  4117,  2007,  3584,  3830,  5537,  4275,  3081,  6819,\n",
      "         3334,  5638,  2023,  4832,  1999,  9762,  5688,  2000,  1996, 12158,\n",
      "         8089,  2008,  1037,  4563,  6685,  4853,  2003,  2007,  2844, 12530,\n",
      "        15266,  2090,  2057,  2265,  2129,  2000,  3857,  1037,  4101,  2944,\n",
      "         1997,  6685, 13543,  3117,  2838,  2008,  2944,  2122, 10266,  2046,\n",
      "         5860, 20026,  3981,  6024,  8833,  4179,  2906,  2023,  2291,  6162,\n",
      "         2015,  2019,  7312,  1997,  2035,  9918,  4563,  9918,  2058,  1037,\n",
      "         2396,  2981,  2465, 18095,  2005,  2751, 21515,  4232, 11968,  3366,\n",
      "         3628,  2006,  1996,  2713,  1997, 21641,  3973,  5754, 17287,  3064,\n",
      "        13058,  6525,  2107,  2004,  4853,  7159,  3802,  1998, 17678,  9299,\n",
      "         3802,  2038,  2081,  2009,  2825,  2000,  4503,  7778,  4275,  2005,\n",
      "        12978, 21641,  2535, 28847,  1998, 10975,  4215,  4819,  3802, 15990,\n",
      "         2063,  1998,  2107,  3001,  2031,  4453,  2195, 12158,  3973, 12774,\n",
      "         2838,  2005,  5860, 20026, 19185,  9918,  1998,  2037, 10873,  2795,\n",
      "         2122,  2838,  2788,  2839,  4697,  5919,  1997,  3265,  9918,  1998,\n",
      "         1996,  2009,  2003, 10358,  2008,  1996, 10873,  1998,  1996,  2838,\n",
      "         1997,  9918,  2024,  3811,  2005,  2045,  2024,  2524, 14679,  2008,\n",
      "         9918,  3685, 17702,  2007,  2169,  2060,  2030,  1996,  1998,  2036,\n",
      "         3730, 14679,  2005,  2003,  2009,  9832,  2008,  1037,  3653, 16467,\n",
      "         2097,  2031,  2048,  2030,  2062,  4005,  2030,  2008,  1037,  3653,\n",
      "        16467,  2109,  1999,  1996,  3161,  2376,  2097,  2031,  1037,  4323,\n",
      "         6685,  3188,  2000,  2019,  4005,  2195,  3001,  2031,  5100,  2107,\n",
      "         2005,  1998, 10975,  4215,  4819,  3802,  5953,  3802,  1998,  2195,\n",
      "         3001,  7864,  1999,  1996,  4207,  4708,  1998,  2057,  2265,  2008,\n",
      "         2045,  2024,  3618, 12154,  2000,  2022,  2018,  2011, 11643,  4101,\n",
      "         2592,  2055,  1037,  6685,  2057, 16599,  1037,  5860, 20026,  3981,\n",
      "         6024,  4101,  2944,  2005, 21641,  2535,  2029, 12374,  2062,  3795,\n",
      "         2838,  1998,  6162,  2015,  6020,  2836,  1999,  7831,  2000,  2000,\n",
      "         3066,  2007,  1996, 15078, 11619,  1997,  1996,  2057, 12666,  8790,\n",
      "         4730,  1998,  2128, 26763,  2075,  2057,  2556,  2836,  3463,  2006,\n",
      "         1996,  2337,  2544,  1997, 17678,  9299,  2006, 11968,  3366,  3628,\n",
      "         2004,  2092,  2004,  3463,  2006,  6882, 11968,  8583,  7013,  2011,\n",
      "        11968,  8043,  5136,  1996,  3940,  1997,  2750,  1996,  2367, 19962,\n",
      "         2696, 13306,  4460,  1997,  1996, 12599,  2057,  6807,  2008,  2169,\n",
      "         3248,  1996,  2168,  2535,  5393,  2011,  1996,  3830,  1999,  1996,\n",
      "         3574,  1997,  2023,  3168,  1997,  1996, 12034,  2057,  2655,  2107,\n",
      "        15672,  6039,  2545,  1997, 21641,  4395,  1998,  2256,  4708,  2445,\n",
      "         1037,  6251,  1998,  1037,  4539,  2000,  2709,  2035,  2107, 15672,\n",
      "         2247,  2007,  2037,  6149,  3568,  2028,  4942, 10230,  2243,  2003,\n",
      "         2000,  2177,  1996,  2616,  1997,  1037,  6251,  2046, 15672,  2030,\n",
      "         2004,  1999,  2087,  3025,  2147,  2006, 21641,  2535,  2057,  7868,\n",
      "         1996,  4598,  1997,  1037,  3584, 11968,  7741,  2944,  2008,  2064,\n",
      "        23911,  1037, 11968,  3366,  3392,  1056,  2000,  2169,  1998,  1996,\n",
      "         4708,  2059,  2003,  2000,  3830,  2169, 13045,  1999,  1996, 11968,\n",
      "         3366,  3392,  2007,  1996, 21641,  2535,  1997,  1996,  7655,  2009,\n",
      "         2030,  2065,  1996,  7655,  2515,  2025,  6039,  2151,  2057,  2079,\n",
      "         6911,  2174,  2008,  1996,  4101,  7705,  1998,  2838,  3818,  2182,\n",
      "         2064,  2036,  2022,  2109,  2043,  2069,  1037,  8467, 11968,  3366,\n",
      "         6630,  2003,  2800,  2004,  1999,  1996,  4207,  4708,  1998,  1999,\n",
      "         1996,   102])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer (you can choose other tokenizers as well)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode articles\n",
    "encoded_articles = [tokenizer.encode(article, add_special_tokens=True, truncation=True, padding='max_length', max_length=512) for article in articles]\n",
    "\n",
    "# Convert to tensors\n",
    "article_sequences = [torch.tensor(seq, dtype=torch.long) for seq in encoded_articles]\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "# Print vocabulary size and some sample sequences\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Sample Sequences: {article_sequences[:2]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
