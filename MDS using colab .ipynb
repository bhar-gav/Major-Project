{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-Ew6MQxnuP-","executionInfo":{"status":"ok","timestamp":1726340259600,"user_tz":-330,"elapsed":24680,"user":{"displayName":"Bhargav Chauhan","userId":"15413947499457026544"}},"outputId":"0e432041-599e-4086-c6a6-071957359eaf"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","\n","# Load BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","bert_model.eval()  # Set BERT to evaluation mode\n","\n","# Load data from CSV\n","data_path = '/content/drive/My Drive/MDS/cleaned_articles.csv'\n","data = pd.read_csv(data_path)\n","\n","# Ensure necessary columns are present\n","if 'row_article' not in data.columns or 'summary' not in data.columns:\n","    raise ValueError(\"Dataframe must contain 'row_article' and 'summary' columns\")\n","\n","# Tokenize and encode data\n","def encode_texts(texts, tokenizer, max_length=512):\n","    encoded = tokenizer(texts.tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n","    return encoded\n","\n","def get_bert_embeddings(encoded, model):\n","    with torch.no_grad():\n","        outputs = model(**encoded)\n","        return outputs.last_hidden_state\n","\n","# Encode articles and summaries\n","article_encoded = encode_texts(data['row_article'], bert_tokenizer)\n","summary_encoded = encode_texts(data['summary'], bert_tokenizer)\n","\n","# Extract BERT embeddings for articles\n","article_embeddings = get_bert_embeddings(article_encoded, bert_model)\n","\n","# Define max length for summaries\n","max_summary_length = 512\n","\n","# Convert summaries to token IDs and pad/truncate\n","def adjust_summary_lengths(summaries, tokenizer, max_length):\n","    adjusted_summaries = []\n","    for text in summaries:\n","        tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n","        if len(tokens) > max_length:\n","            adjusted_summary = tokens[:max_length]  # Truncate\n","        else:\n","            adjusted_summary = tokens + [0] * (max_length - len(tokens))  # Pad\n","        adjusted_summaries.append(adjusted_summary)\n","    return torch.tensor(adjusted_summaries)\n","\n","summary_padded = adjust_summary_lengths(data['summary'], bert_tokenizer, max_summary_length)\n","\n","# Save data\n","torch.save(article_encoded, '/content/drive/My Drive/MDS/article_encoded.pt')\n","torch.save(summary_encoded, '/content/drive/My Drive/MDS/summary_encoded.pt')\n","torch.save(article_embeddings, '/content/drive/My Drive/MDS/article_embeddings.pt')\n","torch.save(summary_padded, '/content/drive/My Drive/MDS/summary_padded.pt')\n","\n","# Debugging: Print sample data\n","print(\"Sample data:\", data.head())\n","print(\"Sample encoded article:\", article_encoded['input_ids'][0])\n","print(\"Sample encoded summary:\", summary_encoded['input_ids'][0])\n","print(\"Sample article embedding shape:\", article_embeddings.shape)\n","print(\"Sample summary padded shape:\", summary_padded.shape)"],"metadata":{"id":"AN1rF1prF9bt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726326678740,"user_tz":-330,"elapsed":1238394,"user":{"displayName":"Bhargav Chauhan","userId":"15413947499457026544"}},"outputId":"fd09e255-4b1f-4e76-cf70-8df2c4440640"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Sample data:    index        id                                        row_article  \\\n","0      0  P95-1021  d tree grammars designed to share some of the ...   \n","1      1  P05-1073  joint learning improves semantic role labeling...   \n","2      2  D09-1127  bilingually constrained monolingual shift redu...   \n","3      3  P02-1017  a generative constituent context model for imp...   \n","4      4  J90-1003  word association norms mutual information and ...   \n","\n","                                             summary  row_article_length  \\\n","0  we define a new grammar formalism called d tre...               40760   \n","1  title joint learning improves semantic role la...               24452   \n","2  conclusion we have presented a novel parsing p...               24503   \n","3  title a generative constituent context model f...               24502   \n","4  we began this paper with the psycholinguistic ...               34693   \n","\n","   summary_length  \n","0             953  \n","1             583  \n","2             776  \n","3             677  \n","4             852  \n","Sample encoded article: tensor([  101,  1040,  3392,  8035,  2015,  2881,  2000,  3745,  2070,  1997,\n","         1996, 12637,  1997,  6415,  2096, 27363,  2070,  2049, 12546,  2048,\n","         5512,  3136,  2170,  4942,  8043,  3508,  1998,  2905, 20621,  3258,\n","         1996,  2087,  8200, 10768,  7113,  2546,  2008,  4406,  6415,  2045,\n","         2003,  3143,  6375,  3012,  1999,  1996,  2126,  2008,  1996, 14396,\n","        16105,  9289,  5167,  4942,  8043,  3508,  2467, 14788,  2000, 13711,\n","         3370,  1998,  2905, 20621,  3258,  2000, 16913,  2072,  7297,  4406,\n","         6415,  3073,  1037,  6375,  4106,  2005,  1059, 14227, 21818,  3672,\n","         1999,  2394,  1998, 13329,  2072,  2750,  1996,  2755,  2008,  1996,\n","         1059,  2232,  5783,  1999, 13329,  2072,  3544,  1999,  6251,  2117,\n","         2597,  1998,  2025,  6251,  3988,  2597,  2004,  1999,  2394,  2057,\n","         9375,  1037,  2047,  8035,  5337,  2964,  2170,  1040,  3392, 18653,\n","         2013,  2147,  2006,  3392, 13562,  8035,  2015,  6415, 26645,  3802,\n","         2632,  3339,  1037, 16183, 11638,  3444,  1997,  6415,  2003,  1996,\n","         3668,  5884,  1997, 10246,  2009,  3640,  2169,  4732,  3252,  2064,\n","         2022,  3378,  2007,  1037, 16105,  9289,  8875,  2004,  1999, 16105,\n","         9289,  3550,  8318,  8490, 26645,  2889,  5144,  3141,  2000,  1996,\n","        16105,  9289,  8875,  2107,  2004,  4942, 16280, 20255,  3989,  3820,\n","         3056,  4127,  1997,  2773,  2344,  8386,  2064,  2022,  5228,  2306,\n","         1996,  4732,  2358,  6820,  2278,  1047,  3217,  2818,  3055,  3581,\n","         2826,  1999,  2804,  3961, 12859,  3085,  2664,  2037, 11416,  6024,\n","         3977,  2003,  7182,  2000,  4070,  2005,  3056, 19962,  2696, 13306,\n","        13352,  2008,  2009,  2038,  2042,  5275,  4682,  3458,  6123,  2489,\n","         8035,  2015, 12935,  2290, 11895, 22669,  3106,  6415,  2174,  2038,\n","         2048, 12546,  2029,  3073,  1996, 14354,  2005,  2023,  1996,  2034,\n","         3291,  6936,  1999,  2930,  2008,  1996,  1997, 20885,  1998, 20621,\n","         3258,  2079,  2025,  4949,  4550,  2135,  3031,  1996,  4262,  1997,\n","        13711,  3370,  1998, 14080,  1037,  2117,  3291,  6936,  1999,  2930,\n","         1015,  1016,  2038,  2000,  2079,  2007,  1996,  1997,  3073, 16478,\n","         2005,  3056, 19962,  2696, 13306, 13352,  1999,  4975, 26718,  2290,\n","         2057,  2031,  2699,  2000,  9462,  2122,  3471,  2096,  3588,  4752,\n","         3406,  2054,  2057,  2156,  2004,  1996,  3145, 12637,  1997,  3327,\n","         2049, 11792,  5884,  1997, 10246,  1999,  2930,  1015,  1017,  2057,\n","         8970,  2070,  1997,  1996,  3145,  2838,  1997,  4863,  2129,  2027,\n","         2024,  3832,  2000,  4769,  3471,  2008,  2057,  2031,  4453,  2007,\n","         1015,  1015, 29280,  2015,  1998, 12530, 15266,  3136,  1997, 20885,\n","         1998, 20621,  3258, 14396,  2048, 16105,  9289,  5167,  2009,  2003,\n","         3568,  3019,  2000, 17841,  2122,  3136,  2004,  7411,  1037,  3622,\n","        12158,  7189,  2090,  1996,  2048, 16105,  9289,  5167,  8419,  1037,\n","         7189,  1997, 13711,  3370,  3653, 16467,  2906, 22850,  4765,  7189,\n","         2030,  1997, 14080,  1999, 11850, 12935,  2290,  2241,  8107,  2122,\n","         4262,  2024,  2069, 24655,  2174,  2027,  5050,  2590, 12158, 26406,\n","         2027,  3073,  1037,  6375,  8278,  2000, 28081,  1998,  2027,  2024,\n","         2004,  8040, 25459,  2229, 11895, 22669,  2807,  7475,  2590,  1999,\n","         2344,  2000,  2490,  7778, 11709,  1999,  2358, 11663, 20875,  7705,\n","         2015,  1998,  6413, 14679,  1999,  2116,  7705,  2015, 13711,  3370,\n","         1998, 14080,  2024,  1999,  2755,  2081, 22990,  3196,  3640,  1037,\n","         3584,  8360,  1042,  3252,  1998, 24394,  8035,  2015,  2156,  1041,\n","         1043, 11463,  7327,  2243,  2997,  2224,  2122, 21951,  2004,  1996,\n","         4054,  3978,  2005, 19962,  2696, 13306,  6630,  2057,  2097,  3582,\n","         1996, 24394,  3906,  1999,  7727,  2000, 13711,  3370,  1998, 14080,\n","         2004,   102])\n","Sample encoded summary: tensor([  101,  2057,  9375,  1037,  2047,  8035,  5337,  2964,  2170,  1040,\n","         3392, 18653,  2013,  2147,  2006,  3392, 13562,  8035,  2015,  6415,\n","        26645,  3802,  2632,  3339,  1037, 16183, 11638,  3444,  1997,  6415,\n","         2003,  1996,  3668,  5884,  1997, 10246,  2009,  3640,  2174,  2027,\n","         5050,  2590, 12158, 26406,  2027,  3073,  1037,  6375,  8278,  2000,\n","        28081,  1998,  2027,  2024,  2004,  8040, 25459,  2229, 11895, 22669,\n","         2807,  7475,  2590,  1999,  2344,  2000,  2490,  7778, 11709,  1999,\n","         2358, 11663, 20875,  7705,  2015,  1998,  6413, 14679,  1999,  2116,\n","         7705,  2015, 13711,  3370,  1998, 14080,  2024,  1999,  2755,  2081,\n","        22990,  3196,  3640,  1037,  3584,  8360,  1042,  3252,  1998, 24394,\n","         8035,  2015,  2156,  1041,  1043, 11463,  7327,  2243,  2997,  2224,\n","         2122, 21951,  2004,  1996,  4054,  3978,  2005, 19962,  2696, 13306,\n","         6630,  1996,  2755,  2008, 20621,  3258,  1998, 20885,  2024,  2109,\n","         1999,  1037, 12158,  3973, 21770, 10624,  6914, 14769,  5450,  2965,\n","         2008,  3115,  6415, 29280,  3628,  2079,  2025,  3073,  1037,  2204,\n","         6630,  1997,  1996, 12530, 15266,  2090,  1996,  2616,  1997,  1996,\n","         6251,  1045,  1041,  1997,  1996,  3653, 16467,  6685,  1998, 14080,\n","         3252,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0])\n","Sample article embedding shape: torch.Size([608, 512, 768])\n","Sample summary padded shape: torch.Size([608, 512])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"G4vT5alLHlpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqozWMH9nTh_","executionInfo":{"status":"ok","timestamp":1726340277436,"user_tz":-330,"elapsed":17838,"user":{"displayName":"Bhargav Chauhan","userId":"15413947499457026544"}},"outputId":"827725a8-81c5-469d-dbcb-63b58a2d5278"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","<ipython-input-3-00f871f92ecc>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  article_encoded = torch.load('/content/drive/My Drive/MDS/article_encoded.pt')\n","<ipython-input-3-00f871f92ecc>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  summary_encoded = torch.load('/content/drive/My Drive/MDS/summary_encoded.pt')\n","<ipython-input-3-00f871f92ecc>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  article_embeddings = torch.load('/content/drive/My Drive/MDS/article_embeddings.pt')\n","<ipython-input-3-00f871f92ecc>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  summary_ids = torch.load('/content/drive/My Drive/MDS/summary_ids.pt')\n","<ipython-input-3-00f871f92ecc>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  summary_padded = torch.load('/content/drive/My Drive/MDS/summary_padded.pt')\n"]},{"output_type":"stream","name":"stdout","text":["Sample data:    index        id                                        row_article  \\\n","0      0  P95-1021  d tree grammars designed to share some of the ...   \n","1      1  P05-1073  joint learning improves semantic role labeling...   \n","2      2  D09-1127  bilingually constrained monolingual shift redu...   \n","3      3  P02-1017  a generative constituent context model for imp...   \n","4      4  J90-1003  word association norms mutual information and ...   \n","\n","                                             summary  row_article_length  \\\n","0  we define a new grammar formalism called d tre...               40760   \n","1  title joint learning improves semantic role la...               24452   \n","2  conclusion we have presented a novel parsing p...               24503   \n","3  title a generative constituent context model f...               24502   \n","4  we began this paper with the psycholinguistic ...               34693   \n","\n","   summary_length  \n","0             953  \n","1             583  \n","2             776  \n","3             677  \n","4             852  \n","Sample encoded article: tensor([  101,  1040,  3392,  8035,  2015,  2881,  2000,  3745,  2070,  1997,\n","         1996, 12637,  1997,  6415,  2096, 27363,  2070,  2049, 12546,  2048,\n","         5512,  3136,  2170,  4942,  8043,  3508,  1998,  2905, 20621,  3258,\n","         1996,  2087,  8200, 10768,  7113,  2546,  2008,  4406,  6415,  2045,\n","         2003,  3143,  6375,  3012,  1999,  1996,  2126,  2008,  1996, 14396,\n","        16105,  9289,  5167,  4942,  8043,  3508,  2467, 14788,  2000, 13711,\n","         3370,  1998,  2905, 20621,  3258,  2000, 16913,  2072,  7297,  4406,\n","         6415,  3073,  1037,  6375,  4106,  2005,  1059, 14227, 21818,  3672,\n","         1999,  2394,  1998, 13329,  2072,  2750,  1996,  2755,  2008,  1996,\n","         1059,  2232,  5783,  1999, 13329,  2072,  3544,  1999,  6251,  2117,\n","         2597,  1998,  2025,  6251,  3988,  2597,  2004,  1999,  2394,  2057,\n","         9375,  1037,  2047,  8035,  5337,  2964,  2170,  1040,  3392, 18653,\n","         2013,  2147,  2006,  3392, 13562,  8035,  2015,  6415, 26645,  3802,\n","         2632,  3339,  1037, 16183, 11638,  3444,  1997,  6415,  2003,  1996,\n","         3668,  5884,  1997, 10246,  2009,  3640,  2169,  4732,  3252,  2064,\n","         2022,  3378,  2007,  1037, 16105,  9289,  8875,  2004,  1999, 16105,\n","         9289,  3550,  8318,  8490, 26645,  2889,  5144,  3141,  2000,  1996,\n","        16105,  9289,  8875,  2107,  2004,  4942, 16280, 20255,  3989,  3820,\n","         3056,  4127,  1997,  2773,  2344,  8386,  2064,  2022,  5228,  2306,\n","         1996,  4732,  2358,  6820,  2278,  1047,  3217,  2818,  3055,  3581,\n","         2826,  1999,  2804,  3961, 12859,  3085,  2664,  2037, 11416,  6024,\n","         3977,  2003,  7182,  2000,  4070,  2005,  3056, 19962,  2696, 13306,\n","        13352,  2008,  2009,  2038,  2042,  5275,  4682,  3458,  6123,  2489,\n","         8035,  2015, 12935,  2290, 11895, 22669,  3106,  6415,  2174,  2038,\n","         2048, 12546,  2029,  3073,  1996, 14354,  2005,  2023,  1996,  2034,\n","         3291,  6936,  1999,  2930,  2008,  1996,  1997, 20885,  1998, 20621,\n","         3258,  2079,  2025,  4949,  4550,  2135,  3031,  1996,  4262,  1997,\n","        13711,  3370,  1998, 14080,  1037,  2117,  3291,  6936,  1999,  2930,\n","         1015,  1016,  2038,  2000,  2079,  2007,  1996,  1997,  3073, 16478,\n","         2005,  3056, 19962,  2696, 13306, 13352,  1999,  4975, 26718,  2290,\n","         2057,  2031,  2699,  2000,  9462,  2122,  3471,  2096,  3588,  4752,\n","         3406,  2054,  2057,  2156,  2004,  1996,  3145, 12637,  1997,  3327,\n","         2049, 11792,  5884,  1997, 10246,  1999,  2930,  1015,  1017,  2057,\n","         8970,  2070,  1997,  1996,  3145,  2838,  1997,  4863,  2129,  2027,\n","         2024,  3832,  2000,  4769,  3471,  2008,  2057,  2031,  4453,  2007,\n","         1015,  1015, 29280,  2015,  1998, 12530, 15266,  3136,  1997, 20885,\n","         1998, 20621,  3258, 14396,  2048, 16105,  9289,  5167,  2009,  2003,\n","         3568,  3019,  2000, 17841,  2122,  3136,  2004,  7411,  1037,  3622,\n","        12158,  7189,  2090,  1996,  2048, 16105,  9289,  5167,  8419,  1037,\n","         7189,  1997, 13711,  3370,  3653, 16467,  2906, 22850,  4765,  7189,\n","         2030,  1997, 14080,  1999, 11850, 12935,  2290,  2241,  8107,  2122,\n","         4262,  2024,  2069, 24655,  2174,  2027,  5050,  2590, 12158, 26406,\n","         2027,  3073,  1037,  6375,  8278,  2000, 28081,  1998,  2027,  2024,\n","         2004,  8040, 25459,  2229, 11895, 22669,  2807,  7475,  2590,  1999,\n","         2344,  2000,  2490,  7778, 11709,  1999,  2358, 11663, 20875,  7705,\n","         2015,  1998,  6413, 14679,  1999,  2116,  7705,  2015, 13711,  3370,\n","         1998, 14080,  2024,  1999,  2755,  2081, 22990,  3196,  3640,  1037,\n","         3584,  8360,  1042,  3252,  1998, 24394,  8035,  2015,  2156,  1041,\n","         1043, 11463,  7327,  2243,  2997,  2224,  2122, 21951,  2004,  1996,\n","         4054,  3978,  2005, 19962,  2696, 13306,  6630,  2057,  2097,  3582,\n","         1996, 24394,  3906,  1999,  7727,  2000, 13711,  3370,  1998, 14080,\n","         2004,   102])\n","Sample encoded summary: tensor([  101,  2057,  9375,  1037,  2047,  8035,  5337,  2964,  2170,  1040,\n","         3392, 18653,  2013,  2147,  2006,  3392, 13562,  8035,  2015,  6415,\n","        26645,  3802,  2632,  3339,  1037, 16183, 11638,  3444,  1997,  6415,\n","         2003,  1996,  3668,  5884,  1997, 10246,  2009,  3640,  2174,  2027,\n","         5050,  2590, 12158, 26406,  2027,  3073,  1037,  6375,  8278,  2000,\n","        28081,  1998,  2027,  2024,  2004,  8040, 25459,  2229, 11895, 22669,\n","         2807,  7475,  2590,  1999,  2344,  2000,  2490,  7778, 11709,  1999,\n","         2358, 11663, 20875,  7705,  2015,  1998,  6413, 14679,  1999,  2116,\n","         7705,  2015, 13711,  3370,  1998, 14080,  2024,  1999,  2755,  2081,\n","        22990,  3196,  3640,  1037,  3584,  8360,  1042,  3252,  1998, 24394,\n","         8035,  2015,  2156,  1041,  1043, 11463,  7327,  2243,  2997,  2224,\n","         2122, 21951,  2004,  1996,  4054,  3978,  2005, 19962,  2696, 13306,\n","         6630,  1996,  2755,  2008, 20621,  3258,  1998, 20885,  2024,  2109,\n","         1999,  1037, 12158,  3973, 21770, 10624,  6914, 14769,  5450,  2965,\n","         2008,  3115,  6415, 29280,  3628,  2079,  2025,  3073,  1037,  2204,\n","         6630,  1997,  1996, 12530, 15266,  2090,  1996,  2616,  1997,  1996,\n","         6251,  1045,  1041,  1997,  1996,  3653, 16467,  6685,  1998, 14080,\n","         3252,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0])\n","Sample article embedding shape: torch.Size([608, 512, 768])\n","Sample summary padded shape: torch.Size([608, 512])\n","Sample article lengths: [512, 512, 512, 512, 512]\n","Sample summary lengths: [512, 512, 512, 512, 512]\n","Batch X shape: torch.Size([4, 512, 768])\n","Batch y shape: torch.Size([4, 512])\n"]}],"source":["\n","import torch\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","\n","# Load BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","bert_model.eval()  # Set BERT to evaluation mode\n","\n","# Load encoded data\n","article_encoded = torch.load('/content/drive/My Drive/MDS/article_encoded.pt')\n","summary_encoded = torch.load('/content/drive/My Drive/MDS/summary_encoded.pt')\n","article_embeddings = torch.load('/content/drive/My Drive/MDS/article_embeddings.pt')\n","summary_ids = torch.load('/content/drive/My Drive/MDS/summary_ids.pt')\n","summary_padded = torch.load('/content/drive/My Drive/MDS/summary_padded.pt')\n","\n","# Load the original data for debugging\n","data_path = '/content/drive/My Drive/MDS/cleaned_articles.csv'\n","data = pd.read_csv(data_path)\n","\n","# Debugging: Print sample data\n","print(\"Sample data:\", data.head())\n","print(\"Sample encoded article:\", article_encoded['input_ids'][0])\n","print(\"Sample encoded summary:\", summary_encoded['input_ids'][0])\n","print(\"Sample article embedding shape:\", article_embeddings.shape)\n","print(\"Sample summary padded shape:\", summary_padded.shape)\n","\n","# Check lengths of articles and summaries\n","article_lengths = [len(seq) for seq in article_encoded['input_ids']]\n","summary_lengths = [len(seq) for seq in summary_encoded['input_ids']]\n","\n","# Print some lengths to debug\n","print(f\"Sample article lengths: {article_lengths[:5]}\")\n","print(f\"Sample summary lengths: {summary_lengths[:5]}\")\n","\n","# Create dataset class\n","class TextDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# Split data into train, validation, and test sets\n","def split_data(X, y, train_size=0.7, val_size=0.15, test_size=0.15):\n","    num_samples = len(X)\n","    indices = torch.randperm(num_samples).tolist()\n","    train_end = int(num_samples * train_size)\n","    val_end = train_end + int(num_samples * val_size)\n","\n","    train_indices = indices[:train_end]\n","    val_indices = indices[train_end:val_end]\n","    test_indices = indices[val_end:]\n","\n","    X_train = torch.stack([X[i] for i in train_indices])\n","    y_train = torch.stack([y[i] for i in train_indices])\n","    X_val = torch.stack([X[i] for i in val_indices])\n","    y_val = torch.stack([y[i] for i in val_indices])\n","    X_test = torch.stack([X[i] for i in test_indices])\n","    y_test = torch.stack([y[i] for i in test_indices])\n","\n","    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n","\n","(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_data(article_embeddings, summary_padded)\n","\n","# Create DataLoaders\n","batch_size = 4\n","\n","train_dataset = TextDataset(X_train, y_train)\n","val_dataset = TextDataset(X_val, y_val)\n","test_dataset = TextDataset(X_test, y_test)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Debugging: Check DataLoader output\n","for X_batch, y_batch in train_dataloader:\n","    print(\"Batch X shape:\", X_batch.shape)\n","    print(\"Batch y shape:\", y_batch.shape)\n","    break  # Remove this break to see more batches"]},{"cell_type":"code","source":["import torch\n","import logging\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO)\n","\n","def save_model(model, vocab_size, embedding_dim, hidden_dim, output_dim, num_epochs, batch_size, model_save_path):\n","    try:\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'vocab_size': output_dim,\n","            'embedding_dim': embedding_dim,\n","            'hidden_dim': hidden_dim,\n","            'output_dim': output_dim,\n","            'num_epochs': num_epochs,\n","            'batch_size': batch_size\n","        }, model_save_path)\n","        logging.info(f\"Model saved to {model_save_path}\")\n","    except Exception as e:\n","        logging.error(f\"Error saving the model: {e}\")\n","        raise\n"],"metadata":{"id":"BJNPoZOrKrpR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","\n","# Load BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","bert_model.eval()  # Set BERT to evaluation mode\n","\n","# Load encoded data\n","article_encoded = torch.load('/content/drive/My Drive/MDS/article_encoded.pt', weights_only=False)\n","summary_encoded = torch.load('/content/drive/My Drive/MDS/summary_encoded.pt', weights_only=False)\n","article_embeddings = torch.load('/content/drive/My Drive/MDS/article_embeddings.pt', weights_only=False)\n","summary_ids = torch.load('/content/drive/My Drive/MDS/summary_ids.pt', weights_only=False)\n","summary_padded = torch.load('/content/drive/My Drive/MDS/summary_padded.pt', weights_only=False)\n","\n","# Define Attention Mechanism\n","class Attention(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(Attention, self).__init__()\n","        self.Wa = nn.Linear(hidden_dim * 2, hidden_dim)\n","        self.wa = nn.Linear(hidden_dim, 1, bias=False)\n","\n","    def forward(self, lstm_output):\n","        scores = self.wa(torch.tanh(self.Wa(lstm_output)))\n","        attention_weights = torch.softmax(scores, dim=1)\n","        context_vector = torch.bmm(attention_weights.transpose(1, 2), lstm_output)\n","        return context_vector.squeeze(1)\n","\n","# Define BiLSTM with Attention Model\n","class BiLSTMWithAttention(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(BiLSTMWithAttention, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n","        self.attention = Attention(hidden_dim)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Output for each token position\n","\n","    def forward(self, x):\n","        lstm_output, _ = self.lstm(x)  # Shape: [batch_size, seq_len, hidden_dim * 2]\n","        # Apply linear layer to every token's LSTM output\n","        output = self.fc(lstm_output)  # Output shape: [batch_size, seq_len, output_dim]\n","        return output\n","\n","# Define model parameters\n","input_dim = bert_model.config.hidden_size  # Size of BERT embeddings (usually 768 for bert-base)\n","hidden_dim = 128\n","output_dim = len(bert_tokenizer)  # Size of the vocabulary for the output layer\n","print(f\"Output dim: {output_dim}\")\n","\n","# Initialize model, loss, optimizer, and scheduler\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BiLSTMWithAttention(input_dim, hidden_dim, output_dim).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Example scheduler\n","\n","# Create dataset\n","class TextDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# Split data into train, validation, and test sets\n","def split_data(X, y, train_size=0.7, val_size=0.15, test_size=0.15):\n","    num_samples = len(X)\n","    indices = torch.randperm(num_samples).tolist()\n","    train_end = int(num_samples * train_size)\n","    val_end = train_end + int(num_samples * val_size)\n","\n","    train_indices = indices[:train_end]\n","    val_indices = indices[train_end:val_end]\n","    test_indices = indices[val_end:]\n","\n","    X_train = torch.stack([X[i] for i in train_indices])\n","    y_train = torch.stack([y[i] for i in train_indices])\n","    X_val = torch.stack([X[i] for i in val_indices])\n","    y_val = torch.stack([y[i] for i in val_indices])\n","    X_test = torch.stack([X[i] for i in test_indices])\n","    y_test = torch.stack([y[i] for i in test_indices])\n","\n","    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n","\n","(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_data(article_embeddings, summary_padded)\n","\n","# Create DataLoaders\n","batch_size = 4\n","num_epochs = 10\n","total_batches=608/batch_size\n","train_dataset = TextDataset(X_train, y_train)\n","val_dataset = TextDataset(X_val, y_val)\n","test_dataset = TextDataset(X_test, y_test)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Debugging: Check DataLoader output\n","for X_batch, y_batch in train_dataloader:\n","    print(\"Batch X shape:\", X_batch.shape)\n","    print(\"Batch y shape:\", y_batch.shape)\n","    break  # Remove this break to see more batches\n","\n","\n","import torch\n","import numpy as np\n","\n","def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, num_epochs, save_path='trained_model.pth'):\n","    # Lists to store losses and accuracies\n","    train_losses = []\n","    val_losses = []\n","    val_accuracies = []\n","\n","    # Ensure the model is in training mode\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        batch_no = 0\n","\n","        # Initialize total loss for training\n","        total_loss = 0\n","        model.train()  # Set model to training mode\n","\n","        for X_batch, y_batch in train_dataloader:\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","            optimizer.zero_grad()\n","\n","            batch_no += 1\n","            # Forward pass\n","            outputs = model(X_batch)\n","            print(f\"Epoch {epoch+1} ---- Training Batch {batch_no}/{len(train_dataloader)}\")\n","\n","            # Reshape outputs and targets for CrossEntropyLoss\n","            batch_size, seq_len = X_batch.size(0), X_batch.size(1)\n","            print(f\"Batch Size: {batch_size}, Sequence Length: {seq_len}\")\n","\n","            outputs = outputs.view(-1, output_dim)  # [batch_size * seq_len, output_dim]\n","            print(f\"Reshaped Outputs shape: {outputs.shape}\")\n","            y_batch = y_batch.view(-1)  # [batch_size * seq_len]\n","            print(f\"Reshaped y_batch shape: {y_batch.shape}\")\n","\n","            # Compute loss\n","            loss = criterion(outputs, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        # Update the learning rate\n","        scheduler.step()\n","\n","        # Compute average training loss\n","        avg_train_loss = total_loss / len(train_dataloader)\n","        train_losses.append(avg_train_loss)\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}\")\n","\n","        # Validation\n","        model.eval()  # Set model to evaluation mode\n","        val_loss = 0\n","        correct_preds = 0\n","        total_preds = 0\n","\n","        with torch.no_grad():\n","            for X_batch, y_batch in val_dataloader:\n","                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","                # Forward pass\n","                outputs = model(X_batch)\n","\n","                # Reshape outputs and targets for CrossEntropyLoss\n","                outputs = outputs.view(-1, output_dim)  # [batch_size * seq_len, output_dim]\n","                y_batch = y_batch.view(-1)  # [batch_size * seq_len]\n","\n","                # Compute loss\n","                loss = criterion(outputs, y_batch)\n","                val_loss += loss.item()\n","\n","                # Compute accuracy\n","                _, predicted = torch.max(outputs, dim=1)\n","                correct_preds += (predicted == y_batch).sum().item()\n","                total_preds += y_batch.size(0)\n","\n","        # Compute average validation loss and accuracy\n","        avg_val_loss = val_loss / len(val_dataloader)\n","        avg_val_accuracy = correct_preds / total_preds\n","        val_losses.append(avg_val_loss)\n","        val_accuracies.append(avg_val_accuracy)\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_accuracy * 100:.2f}%\")\n","        print(f\"Learning Rate: {scheduler.get_last_lr()}\")\n","\n","        # Save the model after each epoch\n","        try:\n","            torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'vocab_size': output_dim,  # Assuming output_dim is the vocabulary size\n","                'embedding_dim': input_dim,  # Size of BERT embeddings\n","                'hidden_dim': hidden_dim,  # Hidden dimension size of LSTM\n","                'num_epochs': num_epochs,\n","                'batch_size': batch_size\n","            }, save_path)\n","            print(f\"Model saved to {save_path}\")\n","        except Exception as e:\n","            print(f\"Error saving the model: {e}\")\n","\n","        # Debug: Check for NaN values in losses\n","        if torch.isnan(torch.tensor(avg_train_loss)) or torch.isnan(torch.tensor(avg_val_loss)):\n","            raise ValueError(\"NaN values encountered in loss.\")\n","\n","    return train_losses, val_losses, val_accuracies\n","\n","import matplotlib.pyplot as plt\n","\n","def plot_metrics(train_losses, val_losses, val_accuracies):\n","    epochs = range(1, len(train_losses) + 1)\n","\n","    plt.figure(figsize=(12, 5))\n","\n","    # Plot training and validation loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, train_losses, label='Training Loss')\n","    plt.plot(epochs, val_losses, label='Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.legend()\n","\n","    # Plot validation accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs, [a * 100 for a in val_accuracies], label='Validation Accuracy', color='orange')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy (%)')\n","    plt.title('Validation Accuracy')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# After training\n","train_losses, val_losses, val_accuracies = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, num_epochs)\n","plot_metrics(train_losses, val_losses, val_accuracies)\n","\n","\n","# Define the model save path\n","model_save_path = '/content/drive/My Drive/MDS/trained_model.pth'\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xp0oEmvZw0nj","executionInfo":{"status":"ok","timestamp":1726344687566,"user_tz":-330,"elapsed":3559977,"user":{"displayName":"Bhargav Chauhan","userId":"15413947499457026544"}},"outputId":"56fccf7f-87d8-44c1-8948-1928e0970d7b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Output dim: 30522\n","Batch X shape: torch.Size([4, 512, 768])\n","Batch y shape: torch.Size([4, 512])\n","Epoch 1/10\n","Epoch 1 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 1 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 1/10, Training Loss: 3.0662887642316727\n","Epoch 1/10, Validation Loss: 2.206442682639412, Validation Accuracy: 73.12%\n","Learning Rate: [0.001]\n","Model saved to trained_model.pth\n","Epoch 2/10\n","Epoch 2 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 2 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 2/10, Training Loss: 2.1479950285403526\n","Epoch 2/10, Validation Loss: 2.080984442130379, Validation Accuracy: 73.25%\n","Learning Rate: [0.001]\n","Model saved to trained_model.pth\n","Epoch 3/10\n","Epoch 3 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 3 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 3/10, Training Loss: 2.0924598970145825\n","Epoch 3/10, Validation Loss: 2.071773736373238, Validation Accuracy: 73.53%\n","Learning Rate: [0.001]\n","Model saved to trained_model.pth\n","Epoch 4/10\n","Epoch 4 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 4 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 4/10, Training Loss: 2.0741008464421067\n","Epoch 4/10, Validation Loss: 2.066872399786244, Validation Accuracy: 73.56%\n","Learning Rate: [0.001]\n","Model saved to trained_model.pth\n","Epoch 5/10\n","Epoch 5 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 5 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 5/10, Training Loss: 2.0500393058652078\n","Epoch 5/10, Validation Loss: 2.081322706263998, Validation Accuracy: 73.66%\n","Learning Rate: [0.0001]\n","Model saved to trained_model.pth\n","Epoch 6/10\n","Epoch 6 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 6 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 6/10, Training Loss: 2.0020642759643983\n","Epoch 6/10, Validation Loss: 2.0624198498933213, Validation Accuracy: 73.73%\n","Learning Rate: [0.0001]\n","Model saved to trained_model.pth\n","Epoch 7/10\n","Epoch 7 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 7 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 7/10, Training Loss: 1.984070385727927\n","Epoch 7/10, Validation Loss: 2.061256787051325, Validation Accuracy: 73.73%\n","Learning Rate: [0.0001]\n","Model saved to trained_model.pth\n","Epoch 8/10\n","Epoch 8 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 8 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 8/10, Training Loss: 1.9861303712720069\n","Epoch 8/10, Validation Loss: 2.0629506162975146, Validation Accuracy: 73.73%\n","Learning Rate: [0.0001]\n","Model saved to trained_model.pth\n","Epoch 9/10\n","Epoch 9 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 9 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 9/10, Training Loss: 1.9743002775673555\n","Epoch 9/10, Validation Loss: 2.0676541898561562, Validation Accuracy: 73.70%\n","Learning Rate: [0.0001]\n","Model saved to trained_model.pth\n","Epoch 10/10\n","Epoch 10 ---- Training Batch 1/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 2/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 3/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 4/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 5/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 6/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 7/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 8/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 9/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 10/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 11/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 12/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 13/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 14/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 15/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 16/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 17/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 18/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 19/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 20/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 21/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 22/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 23/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 24/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 25/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 26/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 27/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 28/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 29/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 30/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 31/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 32/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 33/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 34/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 35/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 36/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 37/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 38/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 39/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 40/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 41/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 42/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 43/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 44/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 45/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 46/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 47/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 48/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 49/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 50/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 51/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 52/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 53/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 54/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 55/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 56/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 57/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 58/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 59/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 60/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 61/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 62/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 63/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 64/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 65/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 66/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 67/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 68/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 69/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 70/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 71/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 72/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 73/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 74/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 75/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 76/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 77/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 78/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 79/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 80/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 81/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 82/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 83/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 84/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 85/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 86/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 87/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 88/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 89/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 90/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 91/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 92/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 93/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 94/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 95/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 96/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 97/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 98/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 99/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 100/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 101/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 102/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 103/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 104/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 105/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 106/107\n","Batch Size: 4, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([2048, 30522])\n","Reshaped y_batch shape: torch.Size([2048])\n","Epoch 10 ---- Training Batch 107/107\n","Batch Size: 1, Sequence Length: 512\n","Reshaped Outputs shape: torch.Size([512, 30522])\n","Reshaped y_batch shape: torch.Size([512])\n","Epoch 10/10, Training Loss: 1.9777032315173997\n","Epoch 10/10, Validation Loss: 2.0676591189011284, Validation Accuracy: 73.71%\n","Learning Rate: [1e-05]\n","Model saved to trained_model.pth\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x500 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","\n","# Define Attention Mechanism\n","class Attention(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(Attention, self).__init__()\n","        self.Wa = nn.Linear(hidden_dim * 2, hidden_dim)\n","        self.wa = nn.Linear(hidden_dim, 1, bias=False)\n","\n","    def forward(self, lstm_output):\n","        scores = self.wa(torch.tanh(self.Wa(lstm_output)))\n","        attention_weights = torch.softmax(scores, dim=1)\n","        context_vector = torch.bmm(attention_weights.transpose(1, 2), lstm_output)\n","        return context_vector.squeeze(1)\n","\n","# Define BiLSTM with Attention Model\n","class BiLSTMWithAttention(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(BiLSTMWithAttention, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n","        self.attention = Attention(hidden_dim)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Output for each token position\n","\n","    def forward(self, x):\n","        lstm_output, _ = self.lstm(x)  # Shape: [batch_size, seq_len, hidden_dim * 2]\n","        # Apply linear layer to every token's LSTM output\n","        output = self.fc(lstm_output)  # Output shape: [batch_size, seq_len, output_dim]\n","        return output\n","\n","\n","# Load the saved model\n","def load_model(model_path):\n","    checkpoint = torch.load(model_path)\n","    model = BiLSTMWithAttention(\n","        input_dim=checkpoint['embedding_dim'],\n","        hidden_dim=checkpoint['hidden_dim'],\n","        output_dim=checkpoint['output_dim']\n","    )\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    return model\n","\n","# Define the model path\n","model_path = '/content/drive/My Drive/MDS/trained_model.pth'\n","model = load_model(model_path)\n","\n","# Move the model to the appropriate device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","model.eval()\n"],"metadata":{"id":"Xe5yjT2CwvYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install rouge-score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RusX6XNaNFal","executionInfo":{"status":"ok","timestamp":1726345416915,"user_tz":-330,"elapsed":10449,"user":{"displayName":"Bhargav Chauhan","userId":"15413947499457026544"}},"outputId":"b01793fc-47de-4540-9e63-a91a7e5888e7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=ea13d0ae3f50f371a8a3c5fffebcc87fee6888d1d508d4335c797a5e6ffe7f10\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","from rouge_score import rouge_scorer\n","import numpy as np\n","\n","# Define the model classes (BiLSTM with Attention)\n","class Attention(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(Attention, self).__init__()\n","        self.Wa = nn.Linear(hidden_dim * 2, hidden_dim)\n","        self.wa = nn.Linear(hidden_dim, 1, bias=False)\n","\n","    def forward(self, lstm_output):\n","        scores = self.wa(torch.tanh(self.Wa(lstm_output)))\n","        attention_weights = torch.softmax(scores, dim=1)\n","        context_vector = torch.bmm(attention_weights.transpose(1, 2), lstm_output)\n","        return context_vector.squeeze(1)\n","\n","class BiLSTMWithAttention(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(BiLSTMWithAttention, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n","        self.attention = Attention(hidden_dim)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Output for each token position\n","\n","    def forward(self, x):\n","        lstm_output, _ = self.lstm(x)  # Shape: [batch_size, seq_len, hidden_dim * 2]\n","        context_vector = self.attention(lstm_output)\n","        output = self.fc(context_vector)  # Output shape: [batch_size, output_dim]\n","        return output\n","\n","# Define a function to load the model\n","def load_model(model_path):\n","    checkpoint = torch.load(model_path)\n","    model = BiLSTMWithAttention(\n","        input_dim=checkpoint['embedding_dim'],\n","        hidden_dim=checkpoint['hidden_dim'],\n","        output_dim=checkpoint['vocab_size']\n","    )\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()  # Set model to evaluation mode\n","    return model\n","\n","# Define a function to generate summaries\n","def generate_summaries(model, dataloader, device):\n","    model.eval()\n","    generated_summaries = []\n","\n","    with torch.no_grad():\n","        for X_batch, _ in dataloader:\n","            X_batch = X_batch.to(device)\n","            outputs = model(X_batch)\n","\n","            # Convert logits to token IDs\n","            summaries = torch.argmax(outputs, dim=-1)  # Get the predicted tokens\n","            generated_summaries.extend(summaries.cpu().numpy())\n","\n","    return generated_summaries\n","\n","# Define a function to decode token IDs into text\n","def decode_tokens(token_ids, tokenizer):\n","    \"\"\"\n","    Decode a list of token IDs into human-readable text using a tokenizer.\n","    Includes debugging steps to ensure proper token decoding.\n","\n","    Args:\n","        token_ids (list or numpy.ndarray): List of token IDs to decode.\n","        tokenizer (BertTokenizer): The tokenizer used for encoding and decoding.\n","\n","    Returns:\n","        List of decoded summaries as strings.\n","    \"\"\"\n","    decoded_summaries = []\n","\n","    if isinstance(token_ids[0], (list, np.ndarray)):\n","        for idx, ids in enumerate(token_ids):\n","            print(f\"\\nDecoding batch index {idx}\")\n","            print(f\"Original Token IDs: {ids[:20]}...\")  # Print the first 20 IDs for inspection\n","\n","            # Remove padding tokens\n","            ids = list(filter(lambda x: x != tokenizer.pad_token_id, ids))\n","            print(f\"Token IDs after removing padding: {ids[:20]}...\")\n","\n","            # Convert IDs to tokens\n","            tokens = tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n","            print(f\"Tokens: {tokens[:20]}...\")\n","\n","            # Convert tokens to string\n","            decoded_text = tokenizer.convert_tokens_to_string(tokens)\n","            print(f\"Decoded Text: {decoded_text[:100]}...\")  # Print the first 100 characters\n","\n","            decoded_summaries.append(decoded_text)\n","    else:\n","        print(f\"\\nDecoding single set of token IDs\")\n","        print(f\"Original Token IDs: {token_ids[:20]}...\")\n","\n","        # Remove padding tokens\n","        token_ids = list(filter(lambda x: x != tokenizer.pad_token_id, token_ids))\n","        print(f\"Token IDs after removing padding: {token_ids[:20]}...\")\n","\n","        # Convert IDs to tokens\n","        tokens = tokenizer.convert_ids_to_tokens(token_ids, skip_special_tokens=True)\n","        print(f\"Tokens: {tokens[:20]}...\")\n","\n","        # Convert tokens to string\n","        decoded_text = tokenizer.convert_tokens_to_string(tokens)\n","        print(f\"Decoded Text: {decoded_text[:100]}...\")\n","\n","        decoded_summaries.append(decoded_text)\n","\n","    return decoded_summaries\n","\n","# Define a function to calculate ROUGE scores\n","def calculate_rouge_scores(generated_summaries, reference_summaries):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n","\n","    for idx, (generated, reference) in enumerate(zip(generated_summaries, reference_summaries)):\n","        print(f\"\\nCalculating ROUGE scores for sample index {idx}\")\n","        print(f\"Generated Summary: {generated}\")\n","        print(f\"Reference Summary: {reference}\")\n","\n","        score = scorer.score(reference, generated)\n","        for key in scores.keys():\n","            scores[key].append(score[key].fmeasure)\n","            print(f\"{key} F-measure: {score[key].fmeasure:.4f}\")\n","\n","    avg_scores = {key: sum(value) / len(value) for key, value in scores.items()}\n","    return avg_scores\n","\n","# Define a function to test the model and evaluate with ROUGE\n","def test_model(model_path, test_dataloader, device, tokenizer):\n","    # Load the model\n","    model = load_model(model_path).to(device)\n","\n","    # Generate summaries\n","    generated_summaries = generate_summaries(model, test_dataloader, device)\n","\n","    # Decode generated summaries\n","    decoded_generated_summaries = decode_tokens(generated_summaries, tokenizer)\n","\n","    # Extract reference summaries from the dataloader\n","    reference_summaries = []\n","    for _, y_batch in test_dataloader:\n","        reference_summaries.extend(y_batch.cpu().numpy())\n","\n","    # Decode reference summaries\n","    decoded_reference_summaries = decode_tokens(reference_summaries, tokenizer)\n","\n","    # Calculate ROUGE scores\n","    rouge_scores = calculate_rouge_scores(decoded_generated_summaries, decoded_reference_summaries)\n","\n","    print(\"ROUGE Scores:\")\n","    for key, score in rouge_scores.items():\n","        print(f\"{key}: {score:.4f}\")\n","\n","# Create dataset class\n","class TextDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# Split data into train, validation, and test sets\n","def split_data(X, y, train_size=0.7, val_size=0.15, test_size=0.15):\n","    num_samples = len(X)\n","    indices = torch.randperm(num_samples).tolist()\n","    train_end = int(num_samples * train_size)\n","    val_end = train_end + int(num_samples * val_size)\n","\n","    train_indices = indices[:train_end]\n","    val_indices = indices[train_end:val_end]\n","    test_indices = indices[val_end:]\n","\n","    X_train = torch.stack([X[i] for i in train_indices])\n","    y_train = torch.stack([y[i] for i in train_indices])\n","    X_val = torch.stack([X[i] for i in val_indices])\n","    y_val = torch.stack([y[i] for i in val_indices])\n","    X_test = torch.stack([X[i] for i in test_indices])\n","    y_test = torch.stack([y[i] for i in test_indices])\n","\n","    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n","\n","# Load encoded data\n","article_embeddings = torch.load('/content/drive/My Drive/MDS/article_embeddings.pt')\n","summary_padded = torch.load('/content/drive/My Drive/MDS/summary_padded.pt')\n","\n","(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_data(article_embeddings, summary_padded)\n","\n","# Create DataLoaders\n","batch_size = 4\n","\n","train_dataset = TextDataset(X_train, y_train)\n","val_dataset = TextDataset(X_val, y_val)\n","test_dataset = TextDataset(X_test, y_test)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Initialize BERT tokenizer\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Device setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Path to model\n","model_save_path = '/content/drive/My Drive/MDS/trained_model.pth'\n","\n","# Test the model and calculate ROUGE scores\n","test_model(model_save_path, test_dataloader, device, bert_tokenizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvttayWBMtjV","executionInfo":{"status":"ok","timestamp":1726349080634,"user_tz":-330,"elapsed":11648,"user":{"displayName":"Bhargav Chauhan","userId":"15413947499457026544"}},"outputId":"272f572c-79b6-4fa3-b82e-cc3b8222fd11"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-20-348441534f2c>:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  article_embeddings = torch.load('/content/drive/My Drive/MDS/article_embeddings.pt')\n","<ipython-input-20-348441534f2c>:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  summary_padded = torch.load('/content/drive/My Drive/MDS/summary_padded.pt')\n","<ipython-input-20-348441534f2c>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(model_path)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Decoding single set of token IDs\n","Original Token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n","Token IDs after removing padding: []...\n","Tokens: []...\n","Decoded Text: ...\n","\n","Decoding batch index 0\n","Original Token IDs: [ 2516  2220  3463  2005  2315  9178  5038  2007 18462  6721  4249  3444\n"," 15946  1998  4773  9412 16105  8663  3736  5910]...\n","Token IDs after removing padding: [2516, 2220, 3463, 2005, 2315, 9178, 5038, 2007, 18462, 6721, 4249, 3444, 15946, 1998, 4773, 9412, 16105, 8663, 3736, 5910]...\n","Tokens: ['title', 'early', 'results', 'for', 'named', 'entity', 'recognition', 'with', 'conditional', 'random', 'fields', 'feature', 'induction', 'and', 'web', 'enhanced', 'lexi', '##con', '##sa', '##bs']...\n","Decoded Text: title early results for named entity recognition with conditional random fields feature induction an...\n","\n","Decoding batch index 1\n","Original Token IDs: [ 1996  3463  2265  2008  6882  9312  2478  4895  8004  6444  2522 10085\n"," 10841 14343  5897  2015  2090 12654  7689  2522]...\n","Token IDs after removing padding: [1996, 3463, 2265, 2008, 6882, 9312, 2478, 4895, 8004, 6444, 2522, 10085, 10841, 14343, 5897, 2015, 2090, 12654, 7689, 2522]...\n","Tokens: ['the', 'results', 'show', 'that', 'automatic', 'evaluation', 'using', 'un', '##ig', '##ram', 'co', '##oc', '##cu', '##rre', '##nce', '##s', 'between', 'summary', 'pairs', 'co']...\n","Decoded Text: the results show that automatic evaluation using unigram cooccurrences between summary pairs correla...\n","\n","Decoding batch index 2\n","Original Token IDs: [ 1999  2023  3259  2057 10580  2129 15078  3001  2000  6807  4372 14162\n","  3672  2064  2022  2109  2000 11598  1996 10640]...\n","Token IDs after removing padding: [1999, 2023, 3259, 2057, 10580, 2129, 15078, 3001, 2000, 6807, 4372, 14162, 3672, 2064, 2022, 2109, 2000, 11598, 1996, 10640]...\n","Tokens: ['in', 'this', 'paper', 'we', 'demonstrate', 'how', 'computational', 'systems', 'to', 'recognize', 'en', '##tail', '##ment', 'can', 'be', 'used', 'to', 'enhance', 'the', 'accuracy']...\n","Decoded Text: in this paper we demonstrate how computational systems to recognize entailment can be used to enhanc...\n","\n","Decoding batch index 3\n","Original Token IDs: [ 2516  3435  4725  2005 16293  2241  3793  4106  7875 20528  6593 16293\n","  2241  4083  1041  1043  2490  9207  6681  2038]...\n","Token IDs after removing padding: [2516, 3435, 4725, 2005, 16293, 2241, 3793, 4106, 7875, 20528, 6593, 16293, 2241, 4083, 1041, 1043, 2490, 9207, 6681, 2038]...\n","Tokens: ['title', 'fast', 'methods', 'for', 'kernel', 'based', 'text', 'analysis', '##ab', '##stra', '##ct', 'kernel', 'based', 'learning', 'e', 'g', 'support', 'vector', 'machines', 'has']...\n","Decoded Text: title fast methods for kernel based text analysisabstract kernel based learning e g support vector m...\n","\n","Decoding batch index 4\n","Original Token IDs: [ 2516  7561  5533 10975 19496  3070  1997  3392  9299  8035  2015  2005\n","  2918 15156  7655  8720  7875 20528  6593  4531]...\n","Token IDs after removing padding: [2516, 7561, 5533, 10975, 19496, 3070, 1997, 3392, 9299, 8035, 2015, 2005, 2918, 15156, 7655, 8720, 7875, 20528, 6593, 4531]...\n","Tokens: ['title', 'error', 'driven', 'pr', '##uni', '##ng', 'of', 'tree', '##bank', 'grammar', '##s', 'for', 'base', 'noun', 'phrase', 'identification', '##ab', '##stra', '##ct', 'finding']...\n","Decoded Text: title error driven pruning of treebank grammars for base noun phrase identificationabstract finding ...\n","\n","Decoding batch index 5\n","Original Token IDs: [ 2057  2556  1037  2512 28689 12589  3016 25253  2944  2008 10776 19653\n","  2015 22822  8458 21382  6903 10708  1997  2169]...\n","Token IDs after removing padding: [2057, 2556, 1037, 2512, 28689, 12589, 3016, 25253, 2944, 2008, 10776, 19653, 2015, 22822, 8458, 21382, 6903, 10708, 1997, 2169]...\n","Tokens: ['we', 'present', 'a', 'non', '##para', '##metric', 'bay', '##esian', 'model', 'that', 'jointly', 'induce', '##s', 'mor', '##ph', '##eme', 'segment', '##ations', 'of', 'each']...\n","Decoded Text: we present a nonparametric bayesian model that jointly induces morpheme segmentations of each langua...\n","\n","Decoding batch index 6\n","Original Token IDs: [ 1996  3937  2801  1997  2023  3921  2003  2000  3342  2035 17636 15672\n","  2008  2031  2042  2464  1999  1996  2773 13115]...\n","Token IDs after removing padding: [1996, 3937, 2801, 1997, 2023, 3921, 2003, 2000, 3342, 2035, 17636, 15672, 2008, 2031, 2042, 2464, 1999, 1996, 2773, 13115]...\n","Tokens: ['the', 'basic', 'idea', 'of', 'this', 'approach', 'is', 'to', 'remember', 'all', 'bilingual', 'phrases', 'that', 'have', 'been', 'seen', 'in', 'the', 'word', 'aligned']...\n","Decoded Text: the basic idea of this approach is to remember all bilingual phrases that have been seen in the word...\n","\n","Decoding batch index 7\n","Original Token IDs: [ 2004  1996  2836  1997  9872 13946  4332  5833  2000  2022  3811  7790\n","  2006  2025  3191  6335  2100  2800  4281  3716]...\n","Token IDs after removing padding: [2004, 1996, 2836, 1997, 9872, 13946, 4332, 5833, 2000, 2022, 3811, 7790, 2006, 2025, 3191, 6335, 2100, 2800, 4281, 3716]...\n","Tokens: ['as', 'the', 'performance', 'of', 'theorem', 'proving', 'turns', '##out', 'to', 'be', 'highly', 'dependent', 'on', 'not', 'read', 'il', '##y', 'available', 'background', 'knowledge']...\n","Decoded Text: as the performance of theorem proving turnsout to be highly dependent on not read ily available back...\n","\n","Decoding batch index 8\n","Original Token IDs: [ 2516  4083 17636 16105  8663  2015  2013 18847  2989  8787 13058  6525\n","  7875 20528  6593  2057  2556  1037  4118  2005]...\n","Token IDs after removing padding: [2516, 4083, 17636, 16105, 8663, 2015, 2013, 18847, 2989, 8787, 13058, 6525, 7875, 20528, 6593, 2057, 2556, 1037, 4118, 2005]...\n","Tokens: ['title', 'learning', 'bilingual', 'lexi', '##con', '##s', 'from', 'mono', '##ling', '##ual', 'corp', '##ora', '##ab', '##stra', '##ct', 'we', 'present', 'a', 'method', 'for']...\n","Decoded Text: title learning bilingual lexicons from monolingual corporaabstract we present a method for learning ...\n","\n","Decoding batch index 9\n","Original Token IDs: [ 2516 16105  9289 21641  3141  2791  2007  6721 10629  7365  7875 20528\n","  6593  2116  3001  2005  8518  2107  2004  3160]...\n","Token IDs after removing padding: [2516, 16105, 9289, 21641, 3141, 2791, 2007, 6721, 10629, 7365, 7875, 20528, 6593, 2116, 3001, 2005, 8518, 2107, 2004, 3160]...\n","Tokens: ['title', 'lexi', '##cal', 'semantic', 'related', '##ness', 'with', 'random', 'graph', 'walks', '##ab', '##stra', '##ct', 'many', 'systems', 'for', 'tasks', 'such', 'as', 'question']...\n","Decoded Text: title lexical semantic relatedness with random graph walksabstract many systems for tasks such as qu...\n","\n","Decoding batch index 10\n","Original Token IDs: [ 1996  6994 23615  2036 22164 16809  9140  8035 14676  1998  6263  7561\n","  3446  2731  2009  2003  2517  1999  9262  1998]...\n","Token IDs after removing padding: [1996, 6994, 23615, 2036, 22164, 16809, 9140, 8035, 14676, 1998, 6263, 7561, 3446, 2731, 2009, 2003, 2517, 1999, 9262, 1998]...\n","Tokens: ['the', 'tool', '##kit', 'also', 'implements', 'suffix', 'array', 'grammar', 'extraction', 'and', 'minimum', 'error', 'rate', 'training', 'it', 'is', 'written', 'in', 'java', 'and']...\n","Decoded Text: the toolkit also implements suffix array grammar extraction and minimum error rate training it is wr...\n","\n","Decoding batch index 11\n","Original Token IDs: [ 2516  8114  3671  2433 11968  7741  2005 22863 28230  4937 20265 14482\n","  8035  7875 20528  6593  2104  4937 20265 14482]...\n","Token IDs after removing padding: [2516, 8114, 3671, 2433, 11968, 7741, 2005, 22863, 28230, 4937, 20265, 14482, 8035, 7875, 20528, 6593, 2104, 4937, 20265, 14482]...\n","Tokens: ['title', 'efficient', 'normal', 'form', 'par', '##sing', 'for', 'comb', '##inatory', 'cat', '##ego', '##rial', 'grammar', '##ab', '##stra', '##ct', 'under', 'cat', '##ego', '##rial']...\n","Decoded Text: title efficient normal form parsing for combinatory categorial grammarabstract under categorial gram...\n","\n","Decoding batch index 12\n","Original Token IDs: [ 2516 17541 13588  4083  2005 17834  5579  1999  4045  3906  7875 20528\n","  6593  2057  8556  6882  5579  1997 23250  2653]...\n","Token IDs after removing padding: [2516, 17541, 13588, 4083, 2005, 17834, 5579, 1999, 4045, 3906, 7875, 20528, 6593, 2057, 8556, 6882, 5579, 1997, 23250, 2653]...\n","Tokens: ['title', 'weakly', 'supervised', 'learning', 'for', 'hedge', 'classification', 'in', 'scientific', 'literature', '##ab', '##stra', '##ct', 'we', 'investigate', 'automatic', 'classification', 'of', 'speculative', 'language']...\n","Decoded Text: title weakly supervised learning for hedge classification in scientific literatureabstract we invest...\n","\n","Decoding batch index 13\n","Original Token IDs: [ 2004  1996  3579  1997  2592 14676  2003  9564  2013 15087  2592  2107\n","  2004  2315  9178  2000 12064  2592  2107  2004]...\n","Token IDs after removing padding: [2004, 1996, 3579, 1997, 2592, 14676, 2003, 9564, 2013, 15087, 2592, 2107, 2004, 2315, 9178, 2000, 12064, 2592, 2107, 2004]...\n","Tokens: ['as', 'the', 'focus', 'of', 'information', 'extraction', 'is', 'shifting', 'from', 'nominal', 'information', 'such', 'as', 'named', 'entity', 'to', 'verbal', 'information', 'such', 'as']...\n","Decoded Text: as the focus of information extraction is shifting from nominal information such as named entity to ...\n","\n","Decoding batch index 14\n","Original Token IDs: [ 2023  3259  7534  2019  2471  4895  6342  4842 11365  2098  4083  9896\n","  2005  6882  5456  1997  2315 11422 24524  1999]...\n","Token IDs after removing padding: [2023, 3259, 7534, 2019, 2471, 4895, 6342, 4842, 11365, 2098, 4083, 9896, 2005, 6882, 5456, 1997, 2315, 11422, 24524, 1999]...\n","Tokens: ['this', 'paper', 'presents', 'an', 'almost', 'un', '##su', '##per', '##vis', '##ed', 'learning', 'algorithm', 'for', 'automatic', 'discovery', 'of', 'named', 'entities', 'nes', 'in']...\n","Decoded Text: this paper presents an almost unsupervised learning algorithm for automatic discovery of named entit...\n","\n","Decoding batch index 15\n","Original Token IDs: [ 2516  1037  3722 14402  2241  2944  2005  4989  2389 18394  7875 20528\n","  6593  2057 16599  1037  2047  3722  2944  2005]...\n","Token IDs after removing padding: [2516, 1037, 3722, 14402, 2241, 2944, 2005, 4989, 2389, 18394, 7875, 20528, 6593, 2057, 16599, 1037, 2047, 3722, 2944, 2005]...\n","Tokens: ['title', 'a', 'simple', 'similarity', 'based', 'model', 'for', 'selection', '##al', 'preferences', '##ab', '##stra', '##ct', 'we', 'propose', 'a', 'new', 'simple', 'model', 'for']...\n","Decoded Text: title a simple similarity based model for selectional preferencesabstract we propose a new simple mo...\n","\n","Decoding batch index 16\n","Original Token IDs: [10061  4427  2011  3025 17463  3217  9623  7741  8107  2000 15488  2102\n","  2023  3259 17146  1037  3117  4013  3676 27965]...\n","Token IDs after removing padding: [10061, 4427, 2011, 3025, 17463, 3217, 9623, 7741, 8107, 2000, 15488, 2102, 2023, 3259, 17146, 1037, 3117, 4013, 3676, 27965]...\n","Tokens: ['abstract', 'inspired', 'by', 'previous', 'prep', '##ro', '##ces', '##sing', 'approaches', 'to', 'sm', '##t', 'this', 'paper', 'proposes', 'a', 'novel', 'pro', '##ba', '##bilis']...\n","Decoded Text: abstract inspired by previous preprocessing approaches to smt this paper proposes a novel probabilis...\n","\n","Decoding batch index 17\n","Original Token IDs: [ 2516  4245  1997  2773 19287  1999  7778  3698  5449  7875 20528  6593\n"," 19287  2019  8114  8278  2090  9530 25690  2271]...\n","Token IDs after removing padding: [2516, 4245, 1997, 2773, 19287, 1999, 7778, 3698, 5449, 7875, 20528, 6593, 19287, 2019, 8114, 8278, 2090, 9530, 25690, 2271]...\n","Tokens: ['title', 'generation', 'of', 'word', 'graphs', 'in', 'statistical', 'machine', 'translation', '##ab', '##stra', '##ct', 'graphs', 'an', 'efficient', 'interface', 'between', 'con', '##tino', '##us']...\n","Decoded Text: title generation of word graphs in statistical machine translationabstract graphs an efficient inter...\n","\n","Decoding batch index 18\n","Original Token IDs: [ 2516  7778 11968  7741  2007  2019  8073 15901  3392 13562  8035  7875\n"," 20528  6593  2057  6848  1996 12637  1997 16105]...\n","Token IDs after removing padding: [2516, 7778, 11968, 7741, 2007, 2019, 8073, 15901, 3392, 13562, 8035, 7875, 20528, 6593, 2057, 6848, 1996, 12637, 1997, 16105]...\n","Tokens: ['title', 'statistical', 'par', '##sing', 'with', 'an', 'automatically', 'extracted', 'tree', 'adjoining', 'grammar', '##ab', '##stra', '##ct', 'we', 'discuss', 'the', 'advantages', 'of', 'lexi']...\n","Decoded Text: title statistical parsing with an automatically extracted tree adjoining grammarabstract we discuss ...\n","\n","Decoding batch index 19\n","Original Token IDs: [ 2057  4225  1037 22073  3388  2241  2006  2773  7159  3565  5054  8583\n","  1037  2172 16325  1998  2236 21641  2944  2084]...\n","Token IDs after removing padding: [2057, 4225, 1037, 22073, 3388, 2241, 2006, 2773, 7159, 3565, 5054, 8583, 1037, 2172, 16325, 1998, 2236, 21641, 2944, 2084]...\n","Tokens: ['we', 'defined', 'a', 'tags', '##et', 'based', 'on', 'word', '##net', 'super', '##sen', '##ses', 'a', 'much', 'simpler', 'and', 'general', 'semantic', 'model', 'than']...\n","Decoded Text: we defined a tagset based on wordnet supersenses a much simpler and general semantic model than word...\n","\n","Decoding batch index 20\n","Original Token IDs: [ 2516  5884  6789  2007  3161  4083  2005  2773  3168  4487 21559  5638\n"," 19696  3508  7875 20528  6593  2043  1037  2773]...\n","Token IDs after removing padding: [2516, 5884, 6789, 2007, 3161, 4083, 2005, 2773, 3168, 4487, 21559, 5638, 19696, 3508, 7875, 20528, 6593, 2043, 1037, 2773]...\n","Tokens: ['title', 'domain', 'adaptation', 'with', 'active', 'learning', 'for', 'word', 'sense', 'di', '##sam', '##bi', '##gua', '##tion', '##ab', '##stra', '##ct', 'when', 'a', 'word']...\n","Decoded Text: title domain adaptation with active learning for word sense disambiguationabstract when a word sense...\n","\n","Decoding batch index 21\n","Original Token IDs: [ 2009  6855  2015  2019  7561  3446  1997  1016  6163  2006  1996  3115\n"," 13866  2497  3231  2275  2029  5836  1017  1017]...\n","Token IDs after removing padding: [2009, 6855, 2015, 2019, 7561, 3446, 1997, 1016, 6163, 2006, 1996, 3115, 13866, 2497, 3231, 2275, 2029, 5836, 1017, 1017]...\n","Tokens: ['it', 'obtain', '##s', 'an', 'error', 'rate', 'of', '2', '67', 'on', 'the', 'standard', 'pt', '##b', 'test', 'set', 'which', 'represents', '3', '3']...\n","Decoded Text: it obtains an error rate of 2 67 on the standard ptb test set which represents 3 3 relative error re...\n","\n","Decoding batch index 22\n","Original Token IDs: [ 7091  1999  2023  3259  1045  3818  1037  2944  2005 12515  1996  2963\n","  2121  1055  3086  2389  2110  2029  2003  2241]...\n","Token IDs after removing padding: [7091, 1999, 2023, 3259, 1045, 3818, 1037, 2944, 2005, 12515, 1996, 2963, 2121, 1055, 3086, 2389, 2110, 2029, 2003, 2241]...\n","Tokens: ['conclusion', 'in', 'this', 'paper', 'i', 'proposed', 'a', 'model', 'for', 'determining', 'the', 'hear', '##er', 's', 'attention', '##al', 'state', 'which', 'is', 'based']...\n","Decoded Text: conclusion in this paper i proposed a model for determining the hearer s attentional state which is ...\n","\n","Decoding batch index 23\n","Original Token IDs: [ 2478 12139  5461  2013  7655 15058  2094  7778  3698  5449  2057  2265\n","  2129 11498  8458 23797  2015  1999  2028  2653]...\n","Token IDs after removing padding: [2478, 12139, 5461, 2013, 7655, 15058, 2094, 7778, 3698, 5449, 2057, 2265, 2129, 11498, 8458, 23797, 2015, 1999, 2028, 2653]...\n","Tokens: ['using', 'alignment', 'techniques', 'from', 'phrase', '##base', '##d', 'statistical', 'machine', 'translation', 'we', 'show', 'how', 'para', '##ph', '##rase', '##s', 'in', 'one', 'language']...\n","Decoded Text: using alignment techniques from phrasebased statistical machine translation we show how paraphrases ...\n","\n","Decoding batch index 24\n","Original Token IDs: [ 8915  8737 13331  2140  1016  8681  9312  8518  2005  2051 11423  2824\n","  1998 15850  4262  1996  3732  1997  2029  2001]...\n","Token IDs after removing padding: [8915, 8737, 13331, 2140, 1016, 8681, 9312, 8518, 2005, 2051, 11423, 2824, 1998, 15850, 4262, 1996, 3732, 1997, 2029, 2001]...\n","Tokens: ['te', '##mp', '##eva', '##l', '2', 'comprises', 'evaluation', 'tasks', 'for', 'time', 'expressions', 'events', 'and', 'temporal', 'relations', 'the', 'latter', 'of', 'which', 'was']...\n","Decoded Text: tempeval 2 comprises evaluation tasks for time expressions events and temporal relations the latter ...\n","\n","Decoding batch index 25\n","Original Token IDs: [ 2516  1996  9530  3363  2230  4207  4708  4083  2000 11487 25840  1998\n","  2037  9531  1999  3019  2653  3793  7875 20528]...\n","Token IDs after removing padding: [2516, 1996, 9530, 3363, 2230, 4207, 4708, 4083, 2000, 11487, 25840, 1998, 2037, 9531, 1999, 3019, 2653, 3793, 7875, 20528]...\n","Tokens: ['title', 'the', 'con', '##ll', '2010', 'shared', 'task', 'learning', 'to', 'detect', 'hedges', 'and', 'their', 'scope', 'in', 'natural', 'language', 'text', '##ab', '##stra']...\n","Decoded Text: title the conll 2010 shared task learning to detect hedges and their scope in natural language texta...\n","\n","Decoding batch index 26\n","Original Token IDs: [7091 2057 2031 2649 1037 7778 2773 8085 3444 1996 2773 7189 8185 2008\n"," 2064 2022 2109 2000 2424 9844]...\n","Token IDs after removing padding: [7091, 2057, 2031, 2649, 1037, 7778, 2773, 8085, 3444, 1996, 2773, 7189, 8185, 2008, 2064, 2022, 2109, 2000, 2424, 9844]...\n","Tokens: ['conclusion', 'we', 'have', 'described', 'a', 'statistical', 'word', 'signature', 'feature', 'the', 'word', 'relation', 'matrix', 'that', 'can', 'be', 'used', 'to', 'find', 'matching']...\n","Decoded Text: conclusion we have described a statistical word signature feature the word relation matrix that can ...\n","\n","Decoding batch index 27\n","Original Token IDs: [ 2516 10507  2290  3565 15900  2015  1999  5387  2098  7778  3698  5449\n","  7875 20528  6593 22863 23207  4818  4937 20265]...\n","Token IDs after removing padding: [2516, 10507, 2290, 3565, 15900, 2015, 1999, 5387, 2098, 7778, 3698, 5449, 7875, 20528, 6593, 22863, 23207, 4818, 4937, 20265]...\n","Tokens: ['title', 'cc', '##g', 'super', '##tag', '##s', 'in', 'factor', '##ed', 'statistical', 'machine', 'translation', '##ab', '##stra', '##ct', 'comb', '##inator', '##ial', 'cat', '##ego']...\n","Decoded Text: title ccg supertags in factored statistical machine translationabstract combinatorial categorial gra...\n","\n","Decoding batch index 28\n","Original Token IDs: [ 2516 29290  4013  3676 27965  4588 10507  2290  8035  2015  2013 11177\n","  2433  2007  3020  2344 16905  7875 20528  6593]...\n","Token IDs after removing padding: [2516, 29290, 4013, 3676, 27965, 4588, 10507, 2290, 8035, 2015, 2013, 11177, 2433, 2007, 3020, 2344, 16905, 7875, 20528, 6593]...\n","Tokens: ['title', 'inducing', 'pro', '##ba', '##bilis', '##tic', 'cc', '##g', 'grammar', '##s', 'from', 'logical', 'form', 'with', 'higher', 'order', 'unification', '##ab', '##stra', '##ct']...\n","Decoded Text: title inducing probabilistic ccg grammars from logical form with higher order unificationabstract th...\n","\n","Decoding batch index 29\n","Original Token IDs: [ 1058  5468  3640  2019 11552  5576  3419 19092  3471  2008  7461  3130\n","  4225  9324  9312  5761  4297  7630  2094 13749]...\n","Token IDs after removing padding: [1058, 5468, 3640, 2019, 11552, 5576, 3419, 19092, 3471, 2008, 7461, 3130, 4225, 9324, 9312, 5761, 4297, 7630, 2094, 13749]...\n","Tokens: ['v', 'measure', 'provides', 'an', 'elegant', 'solution', 'tom', '##any', 'problems', 'that', 'affect', 'previously', 'defined', 'cluster', 'evaluation', 'measures', 'inc', '##lu', '##d', 'ing']...\n","Decoded Text: v measure provides an elegant solution tomany problems that affect previously defined cluster evalua...\n","\n","Decoding batch index 30\n","Original Token IDs: [ 1999  2755  2027  2024  1997 10889  2210  5770  2130  2005 10349  2731\n","  1998  3231  2951  9268  2068  2013  1996  2944]...\n","Token IDs after removing padding: [1999, 2755, 2027, 2024, 1997, 10889, 2210, 5770, 2130, 2005, 10349, 2731, 1998, 3231, 2951, 9268, 2068, 2013, 1996, 2944]...\n","Tokens: ['in', 'fact', 'they', 'are', 'of', 'surprisingly', 'little', 'benefit', 'even', 'for', 'matched', 'training', 'and', 'test', 'data', 'removing', 'them', 'from', 'the', 'model']...\n","Decoded Text: in fact they are of surprisingly little benefit even for matched training and test data removing the...\n","\n","Decoding batch index 31\n","Original Token IDs: [ 2516 24048  1037  7778  2112  1997  4613  6415  4590  7875 20528  6593\n"," 13012 13113  2015  1050 22073 24048  2003  2019]...\n","Token IDs after removing padding: [2516, 24048, 1037, 7778, 2112, 1997, 4613, 6415, 4590, 7875, 20528, 6593, 13012, 13113, 2015, 1050, 22073, 24048, 2003, 2019]...\n","Tokens: ['title', 'tnt', 'a', 'statistical', 'part', 'of', 'speech', 'tag', '##ger', '##ab', '##stra', '##ct', 'tri', '##gram', '##s', 'n', 'tags', 'tnt', 'is', 'an']...\n","Decoded Text: title tnt a statistical part of speech taggerabstract trigrams n tags tnt is an efficient statistica...\n","\n","Decoding batch index 32\n","Original Token IDs: [ 3729  2290 11968  7741  2003  5337  3550  2004  1037 27142  9967  3291\n","  2058  1037 10713  5884  2061  2008  8114 27142]...\n","Token IDs after removing padding: [3729, 2290, 11968, 7741, 2003, 5337, 3550, 2004, 1037, 27142, 9967, 3291, 2058, 1037, 10713, 5884, 2061, 2008, 8114, 27142]...\n","Tokens: ['cd', '##g', 'par', '##sing', 'is', 'formal', '##ized', 'as', 'a', 'constraint', 'satisfaction', 'problem', 'over', 'a', 'finite', 'domain', 'so', 'that', 'efficient', 'constraint']...\n","Decoded Text: cdg parsing is formalized as a constraint satisfaction problem over a finite domain so that efficien...\n","\n","Decoding batch index 33\n","Original Token IDs: [ 2057  2556  1037  2155  1997 13792  2000 24134  4621 24155  2015  2057\n","  3231  1996  4118  2006  1996  4708  1997  2440]...\n","Token IDs after removing padding: [2057, 2556, 1037, 2155, 1997, 13792, 2000, 24134, 4621, 24155, 2015, 2057, 3231, 1996, 4118, 2006, 1996, 4708, 1997, 2440]...\n","Tokens: ['we', 'present', 'a', 'family', 'of', 'algorithms', 'to', 'compute', 'effective', 'estimation', '##s', 'we', 'test', 'the', 'method', 'on', 'the', 'task', 'of', 'full']...\n","Decoded Text: we present a family of algorithms to compute effective estimations we test the method on the task of...\n","\n","Decoding batch index 34\n","Original Token IDs: [ 2348  4487  2015 19792  7542  2038  2042  2109  1999  2195 16905 15058\n","  2094  8035  5337 22556  4493  4725  1997 16905]...\n","Token IDs after removing padding: [2348, 4487, 2015, 19792, 7542, 2038, 2042, 2109, 1999, 2195, 16905, 15058, 2094, 8035, 5337, 22556, 4493, 4725, 1997, 16905]...\n","Tokens: ['although', 'di', '##s', '##jun', '##ction', 'has', 'been', 'used', 'in', 'several', 'unification', '##base', '##d', 'grammar', 'formal', '##isms', 'existing', 'methods', 'of', 'unification']...\n","Decoded Text: although disjunction has been used in several unificationbased grammar formalisms existing methods o...\n","\n","Decoding batch index 35\n","Original Token IDs: [ 2057  2224  1996  2529 26186  1997  1996  3001  2000 17908  6882  9312\n"," 12046  2015  2005  5449  3737  1998  2057  3189]...\n","Token IDs after removing padding: [2057, 2224, 1996, 2529, 26186, 1997, 1996, 3001, 2000, 17908, 6882, 9312, 12046, 2015, 2005, 5449, 3737, 1998, 2057, 3189]...\n","Tokens: ['we', 'use', 'the', 'human', 'judgments', 'of', 'the', 'systems', 'to', 'analyze', 'automatic', 'evaluation', 'metric', '##s', 'for', 'translation', 'quality', 'and', 'we', 'report']...\n","Decoded Text: we use the human judgments of the systems to analyze automatic evaluation metrics for translation qu...\n","\n","Decoding batch index 36\n","Original Token IDs: [ 2516  1037  6653  2241  2291  2005  4101  2112  1997  4613  6415  4726\n","  1998 12599  2512 27473 24394 11968  7741  7875]...\n","Token IDs after removing padding: [2516, 1037, 6653, 2241, 2291, 2005, 4101, 2112, 1997, 4613, 6415, 4726, 1998, 12599, 2512, 27473, 24394, 11968, 7741, 7875]...\n","Tokens: ['title', 'a', 'transition', 'based', 'system', 'for', 'joint', 'part', 'of', 'speech', 'tag', '##ging', 'and', 'labeled', 'non', 'projective', 'dependency', 'par', '##sing', '##ab']...\n","Decoded Text: title a transition based system for joint part of speech tagging and labeled non projective dependen...\n","\n","Decoding batch index 37\n","Original Token IDs: [ 2516 23208  4180  4989  1999  7680  7849  3989  1996 11918  4118  7875\n"," 20528  6593  2057  2556  2019 17537  2135 16764]...\n","Token IDs after removing padding: [2516, 23208, 4180, 4989, 1999, 7680, 7849, 3989, 1996, 11918, 4118, 7875, 20528, 6593, 2057, 2556, 2019, 17537, 2135, 16764]...\n","Tokens: ['title', 'evaluating', 'content', 'selection', 'in', 'sum', '##mar', '##ization', 'the', 'pyramid', 'method', '##ab', '##stra', '##ct', 'we', 'present', 'an', 'empirical', '##ly', 'grounded']...\n","Decoded Text: title evaluating content selection in summarization the pyramid methodabstract we present an empiric...\n","\n","Decoding batch index 38\n","Original Token IDs: [ 2516  2131  2041  1996  3789 12515  2490  2030  4559  2013  7740  2723\n","  5981 24051  3736  5910  6494  6593  2057  8556]...\n","Token IDs after removing padding: [2516, 2131, 2041, 1996, 3789, 12515, 2490, 2030, 4559, 2013, 7740, 2723, 5981, 24051, 3736, 5910, 6494, 6593, 2057, 8556]...\n","Tokens: ['title', 'get', 'out', 'the', 'vote', 'determining', 'support', 'or', 'opposition', 'from', 'congressional', 'floor', 'debate', 'transcript', '##sa', '##bs', '##tra', '##ct', 'we', 'investigate']...\n","Decoded Text: title get out the vote determining support or opposition from congressional floor debate transcripts...\n","\n","Decoding batch index 39\n","Original Token IDs: [ 2516  4895  6342  4842 11365  2098 11643  1997 10474 11450  7875 20528\n","  6593  2057 16599  1996  2034  4895  6342  4842]...\n","Token IDs after removing padding: [2516, 4895, 6342, 4842, 11365, 2098, 11643, 1997, 10474, 11450, 7875, 20528, 6593, 2057, 16599, 1996, 2034, 4895, 6342, 4842]...\n","Tokens: ['title', 'un', '##su', '##per', '##vis', '##ed', 'modeling', 'of', 'twitter', 'conversations', '##ab', '##stra', '##ct', 'we', 'propose', 'the', 'first', 'un', '##su', '##per']...\n","Decoded Text: title unsupervised modeling of twitter conversationsabstract we propose the first unsupervised appro...\n","\n","Decoding batch index 40\n","Original Token IDs: [1996 8089 2369 1996 3921 2008 2616 2383 1996 2168 5449 2411 3745 2070\n"," 9812 1997 3574 5260 2000 2396]...\n","Token IDs after removing padding: [1996, 8089, 2369, 1996, 3921, 2008, 2616, 2383, 1996, 2168, 5449, 2411, 3745, 2070, 9812, 1997, 3574, 5260, 2000, 2396]...\n","Tokens: ['the', 'observation', 'behind', 'the', 'approach', 'that', 'words', 'having', 'the', 'same', 'translation', 'often', 'share', 'some', 'dimension', 'of', 'meaning', 'leads', 'to', 'art']...\n","Decoded Text: the observation behind the approach that words having the same translation often share some dimensio...\n","\n","Decoding batch index 41\n","Original Token IDs: [ 2516  9229  4895  6342  4842 11365  2098 24394 11968  7741  2007 26108\n"," 18046  1998 27045  7875 20528  6593  4895  6342]...\n","Token IDs after removing padding: [2516, 9229, 4895, 6342, 4842, 11365, 2098, 24394, 11968, 7741, 2007, 26108, 18046, 1998, 27045, 7875, 20528, 6593, 4895, 6342]...\n","Tokens: ['title', 'improving', 'un', '##su', '##per', '##vis', '##ed', 'dependency', 'par', '##sing', 'with', 'richer', 'contexts', 'and', 'smoothing', '##ab', '##stra', '##ct', 'un', '##su']...\n","Decoded Text: title improving unsupervised dependency parsing with richer contexts and smoothingabstract unsupervi...\n","\n","Decoding batch index 42\n","Original Token IDs: [ 2060  4013  3676 27965  4588  4275  1997  2653  2071 19274  1996  3754\n","  2000 23032 19962 13462  2015  2030 10425  1997]...\n","Token IDs after removing padding: [2060, 4013, 3676, 27965, 4588, 4275, 1997, 2653, 2071, 19274, 1996, 3754, 2000, 23032, 19962, 13462, 2015, 2030, 10425, 1997]...\n","Tokens: ['other', 'pro', '##ba', '##bilis', '##tic', 'models', 'of', 'language', 'could', 'insert', 'the', 'ability', 'to', 'query', 'syn', '##set', '##s', 'or', 'paths', 'of']...\n","Decoded Text: other probabilistic models of language could insert the ability to query synsets or paths of wordnet...\n","\n","Decoding batch index 43\n","Original Token IDs: [ 2064  7588  4553  2000  6709  2029 11746  6118 16636  1037  3327  7339\n","  2057  4503  7778  4275  2000  5425  2129 15251]...\n","Token IDs after removing padding: [2064, 7588, 4553, 2000, 6709, 2029, 11746, 6118, 16636, 1037, 3327, 7339, 2057, 4503, 7778, 4275, 2000, 5425, 2129, 15251]...\n","Tokens: ['can', 'computers', 'learn', 'to', 'identify', 'which', 'sentences', 'strongly', 'convey', 'a', 'particular', 'perspective', 'we', 'develop', 'statistical', 'models', 'to', 'capture', 'how', 'perspectives']...\n","Decoded Text: can computers learn to identify which sentences strongly convey a particular perspective we develop ...\n","\n","Decoding batch index 44\n","Original Token IDs: [ 2516  6234  2132 11968  7741  2005  2653  4275  7875 20528  6593  2057\n","  2556  2048  2653  4275  2241  2588  2019  1037]...\n","Token IDs after removing padding: [2516, 6234, 2132, 11968, 7741, 2005, 2653, 4275, 7875, 20528, 6593, 2057, 2556, 2048, 2653, 4275, 2241, 2588, 2019, 1037]...\n","Tokens: ['title', 'immediate', 'head', 'par', '##sing', 'for', 'language', 'models', '##ab', '##stra', '##ct', 'we', 'present', 'two', 'language', 'models', 'based', 'upon', 'an', 'a']...\n","Decoded Text: title immediate head parsing for language modelsabstract we present two language models based upon a...\n","\n","Decoding batch index 45\n","Original Token IDs: [ 2516  1037  5254 26351  8093 17175  2271  4563 25523  5813  9896  2241\n","  2006  1996  4330  3392  7875 20528  6593  2023]...\n","Token IDs after removing padding: [2516, 1037, 5254, 26351, 8093, 17175, 2271, 4563, 25523, 5813, 9896, 2241, 2006, 1996, 4330, 3392, 7875, 20528, 6593, 2023]...\n","Tokens: ['title', 'a', 'mention', 'sync', '##hr', '##ono', '##us', 'core', '##ference', 'resolution', 'algorithm', 'based', 'on', 'the', 'bell', 'tree', '##ab', '##stra', '##ct', 'this']...\n","Decoded Text: title a mention synchronous coreference resolution algorithm based on the bell treeabstract this pap...\n","\n","Decoding batch index 46\n","Original Token IDs: [ 2478  1996  5421  4664  1997  1996  2822  3392  9299  2256  2944  2003\n","  4738  2009 25284  2135  2000 25845  1996 14785]...\n","Token IDs after removing padding: [2478, 1996, 5421, 4664, 1997, 1996, 2822, 3392, 9299, 2256, 2944, 2003, 4738, 2009, 25284, 2135, 2000, 25845, 1996, 14785]...\n","Tokens: ['using', 'the', 'translated', 'portion', 'of', 'the', 'chinese', 'tree', '##bank', 'our', 'model', 'is', 'trained', 'it', '##erative', '##ly', 'to', 'maximize', 'the', 'marginal']...\n","Decoded Text: using the translated portion of the chinese treebank our model is trained iteratively to maximize th...\n","\n","Decoding batch index 47\n","Original Token IDs: [ 2023  2944  2041  4842 22694  1996 26163 10910  1037 12599 11718  1998\n","  9131  1997  2039  2000  6356  2023  7127  2008]...\n","Token IDs after removing padding: [2023, 2944, 2041, 4842, 22694, 1996, 26163, 10910, 1037, 12599, 11718, 1998, 9131, 1997, 2039, 2000, 6356, 2023, 7127, 2008]...\n","Tokens: ['this', 'model', 'out', '##per', '##forms', 'the', 'baseline', 'achieving', 'a', 'labeled', 'precision', 'and', 'recall', 'of', 'up', 'to', '74', 'this', 'indicates', 'that']...\n","Decoded Text: this model outperforms the baseline achieving a labeled precision and recall of up to 74 this indica...\n","\n","Decoding batch index 48\n","Original Token IDs: [ 2516  3929  6882 16105  8663  4935  2005  5884  8048 15792  4106  7875\n"," 20528  6593  2023  3259 17146  2019  4895  6342]...\n","Token IDs after removing padding: [2516, 3929, 6882, 16105, 8663, 4935, 2005, 5884, 8048, 15792, 4106, 7875, 20528, 6593, 2023, 3259, 17146, 2019, 4895, 6342]...\n","Tokens: ['title', 'fully', 'automatic', 'lexi', '##con', 'expansion', 'for', 'domain', 'oriented', 'sentiment', 'analysis', '##ab', '##stra', '##ct', 'this', 'paper', 'proposes', 'an', 'un', '##su']...\n","Decoded Text: title fully automatic lexicon expansion for domain oriented sentiment analysisabstract this paper pr...\n","\n","Decoding batch index 49\n","Original Token IDs: [ 2516  6882 26384  1998  9324  2075  1997  2714  2616  7875 20528  6593\n","  6879  6494 14853 28081  2013  3793  2003  2028]...\n","Token IDs after removing padding: [2516, 6882, 26384, 1998, 9324, 2075, 1997, 2714, 2616, 7875, 20528, 6593, 6879, 6494, 14853, 28081, 2013, 3793, 2003, 2028]...\n","Tokens: ['title', 'automatic', 'retrieval', 'and', 'cluster', '##ing', 'of', 'similar', 'words', '##ab', '##stra', '##ct', 'boots', '##tra', '##pping', 'semantics', 'from', 'text', 'is', 'one']...\n","Decoded Text: title automatic retrieval and clustering of similar wordsabstract bootstrapping semantics from text ...\n","\n","Decoding batch index 50\n","Original Token IDs: [ 2009  4332  2041  2008  2122 18960 11968  8043  2015  2064  2079  2087\n","  1997  1996  2147  3223  2000  3345  1998  6611]...\n","Token IDs after removing padding: [2009, 4332, 2041, 2008, 2122, 18960, 11968, 8043, 2015, 2064, 2079, 2087, 1997, 1996, 2147, 3223, 2000, 3345, 1998, 6611]...\n","Tokens: ['it', 'turns', 'out', 'that', 'these', 'generalized', 'par', '##ser', '##s', 'can', 'do', 'most', 'of', 'the', 'work', 'required', 'to', 'train', 'and', 'apply']...\n","Decoded Text: it turns out that these generalized parsers can do most of the work required to train and apply a sy...\n","\n","Decoding batch index 51\n","Original Token IDs: [ 2516 17636 11968  7741  2007  5387  2098 24155  2478  2394  2000 11968\n","  3366  4759  7875 20528  6593  2057  6235  2129]...\n","Token IDs after removing padding: [2516, 17636, 11968, 7741, 2007, 5387, 2098, 24155, 2478, 2394, 2000, 11968, 3366, 4759, 7875, 20528, 6593, 2057, 6235, 2129]...\n","Tokens: ['title', 'bilingual', 'par', '##sing', 'with', 'factor', '##ed', 'estimation', 'using', 'english', 'to', 'par', '##se', 'korean', '##ab', '##stra', '##ct', 'we', 'describe', 'how']...\n","Decoded Text: title bilingual parsing with factored estimation using english to parse koreanabstract we describe h...\n","\n","Decoding batch index 52\n","Original Token IDs: [ 2057  2224  2256  3399  2000  8970  1037  7399  9896  2008  2064  2022\n","  2109  2000 18547  2013  2773 13115  5903 13058]...\n","Token IDs after removing padding: [2057, 2224, 2256, 3399, 2000, 8970, 1037, 7399, 9896, 2008, 2064, 2022, 2109, 2000, 18547, 2013, 2773, 13115, 5903, 13058]...\n","Tokens: ['we', 'use', 'our', 'theory', 'to', 'introduce', 'a', 'linear', 'algorithm', 'that', 'can', 'be', 'used', 'to', 'derive', 'from', 'word', 'aligned', 'parallel', 'corp']...\n","Decoded Text: we use our theory to introduce a linear algorithm that can be used to derive from word aligned paral...\n","\n","Decoding batch index 53\n","Original Token IDs: [ 2516  1037  4101  2944  1997  3793  1998  7814  8599  2005 15792  7680\n","  7849  3989  7875 20528  6593  3784  4391  2024]...\n","Token IDs after removing padding: [2516, 1037, 4101, 2944, 1997, 3793, 1998, 7814, 8599, 2005, 15792, 7680, 7849, 3989, 7875, 20528, 6593, 3784, 4391, 2024]...\n","Tokens: ['title', 'a', 'joint', 'model', 'of', 'text', 'and', 'aspect', 'ratings', 'for', 'sentiment', 'sum', '##mar', '##ization', '##ab', '##stra', '##ct', 'online', 'reviews', 'are']...\n","Decoded Text: title a joint model of text and aspect ratings for sentiment summarizationabstract online reviews ar...\n","\n","Decoding batch index 54\n","Original Token IDs: [ 2057  2036  6848  2129  1996  8893  4245  2944  2064  2022  2109  2000\n"," 21934 28250  2783 16937  1998 11598  2037  3417]...\n","Token IDs after removing padding: [2057, 2036, 6848, 2129, 1996, 8893, 4245, 2944, 2064, 2022, 2109, 2000, 21934, 28250, 2783, 16937, 1998, 11598, 2037, 3417]...\n","Tokens: ['we', 'also', 'discuss', 'how', 'the', 'hybrid', 'generation', 'model', 'can', 'be', 'used', 'to', 'sim', '##plify', 'current', 'generators', 'and', 'enhance', 'their', 'port']...\n","Decoded Text: we also discuss how the hybrid generation model can be used to simplify current generators and enhan...\n","\n","Decoding batch index 55\n","Original Token IDs: [ 2057  3273  1996  3466  1997  5815  2195  3798  2000  2023 16690  2094\n","  2465 18095  1998  2057  2179  2008  2130  1996]...\n","Token IDs after removing padding: [2057, 3273, 1996, 3466, 1997, 5815, 2195, 3798, 2000, 2023, 16690, 2094, 2465, 18095, 1998, 2057, 2179, 2008, 2130, 1996]...\n","Tokens: ['we', 'studied', 'the', 'effect', 'of', 'adding', 'several', 'levels', 'to', 'this', 'cascade', '##d', 'class', '##ifier', 'and', 'we', 'found', 'that', 'even', 'the']...\n","Decoded Text: we studied the effect of adding several levels to this cascaded classifier and we found that even th...\n","\n","Decoding batch index 56\n","Original Token IDs: [ 2516  2488  9312  2005 24402  7561 18140  7875 20528  6593  2057  2556\n","  1037  3117  4118  2005 23208 24402  7561 18140]...\n","Token IDs after removing padding: [2516, 2488, 9312, 2005, 24402, 7561, 18140, 7875, 20528, 6593, 2057, 2556, 1037, 3117, 4118, 2005, 23208, 24402, 7561, 18140]...\n","Tokens: ['title', 'better', 'evaluation', 'for', 'grammatical', 'error', 'correction', '##ab', '##stra', '##ct', 'we', 'present', 'a', 'novel', 'method', 'for', 'evaluating', 'grammatical', 'error', 'correction']...\n","Decoded Text: title better evaluation for grammatical error correctionabstract we present a novel method for evalu...\n","\n","Decoding batch index 57\n","Original Token IDs: [ 2516  2822  6903  3370  1998  2047  2773 10788  2478 18462  6721  4249\n","  7875 20528  6593  2822  2773  6903  3370  2003]...\n","Token IDs after removing padding: [2516, 2822, 6903, 3370, 1998, 2047, 2773, 10788, 2478, 18462, 6721, 4249, 7875, 20528, 6593, 2822, 2773, 6903, 3370, 2003]...\n","Tokens: ['title', 'chinese', 'segment', '##ation', 'and', 'new', 'word', 'detection', 'using', 'conditional', 'random', 'fields', '##ab', '##stra', '##ct', 'chinese', 'word', 'segment', '##ation', 'is']...\n","Decoded Text: title chinese segmentation and new word detection using conditional random fieldsabstract chinese wo...\n","\n","Decoding batch index 58\n","Original Token IDs: [ 2516  2146  3292 24394  5813  1999  8073  3734  2898  6325  7473  2546\n","  2290  2241  1048  2546  2290 20167  3736  5910]...\n","Token IDs after removing padding: [2516, 2146, 3292, 24394, 5813, 1999, 8073, 3734, 2898, 6325, 7473, 2546, 2290, 2241, 1048, 2546, 2290, 20167, 3736, 5910]...\n","Tokens: ['title', 'long', 'distance', 'dependency', 'resolution', 'in', 'automatically', 'acquired', 'wide', 'coverage', 'pc', '##f', '##g', 'based', 'l', '##f', '##g', 'approximation', '##sa', '##bs']...\n","Decoded Text: title long distance dependency resolution in automatically acquired wide coverage pcfg based lfg app...\n","\n","Decoding batch index 59\n","Original Token IDs: [ 2516  4776 21682  8332 13827  1999  4800  2989  8787 18215  8035 15946\n","  7875 20528  6593  2034  2265  2129  1037  8332]...\n","Token IDs after removing padding: [2516, 4776, 21682, 8332, 13827, 1999, 4800, 2989, 8787, 18215, 8035, 15946, 7875, 20528, 6593, 2034, 2265, 2129, 1037, 8332]...\n","Tokens: ['title', 'anne', '##aling', 'structural', 'bias', 'in', 'multi', '##ling', '##ual', 'weighted', 'grammar', 'induction', '##ab', '##stra', '##ct', 'first', 'show', 'how', 'a', 'structural']...\n","Decoded Text: title annealing structural bias in multilingual weighted grammar inductionabstract first show how a ...\n","\n","Decoding batch index 60\n","Original Token IDs: [ 2516  2224  1997  2490  9207  4083  2005 20000  8720  7875 20528  6593\n","  7091  2057  2031  4162  2256  3818  4118  2000]...\n","Token IDs after removing padding: [2516, 2224, 1997, 2490, 9207, 4083, 2005, 20000, 8720, 7875, 20528, 6593, 7091, 2057, 2031, 4162, 2256, 3818, 4118, 2000]...\n","Tokens: ['title', 'use', 'of', 'support', 'vector', 'learning', 'for', 'chunk', 'identification', '##ab', '##stra', '##ct', 'conclusion', 'we', 'have', 'applied', 'our', 'proposed', 'method', 'to']...\n","Decoded Text: title use of support vector learning for chunk identificationabstract conclusion we have applied our...\n","\n","Decoding batch index 61\n","Original Token IDs: [ 2516  2478 19962  2696 13306 24394  2004  2334  6123  2000 10663  2773\n","  3168 27637  7875 20528  6593  2087  3025 13931]...\n","Token IDs after removing padding: [2516, 2478, 19962, 2696, 13306, 24394, 2004, 2334, 6123, 2000, 10663, 2773, 3168, 27637, 7875, 20528, 6593, 2087, 3025, 13931]...\n","Tokens: ['title', 'using', 'syn', '##ta', '##ctic', 'dependency', 'as', 'local', 'context', 'to', 'resolve', 'word', 'sense', 'ambiguity', '##ab', '##stra', '##ct', 'most', 'previous', 'corpus']...\n","Decoded Text: title using syntactic dependency as local context to resolve word sense ambiguityabstract most previ...\n","\n","Decoding batch index 62\n","Original Token IDs: [ 2516  4083  1996  9531  1997 17834 23391  1999 20906  6981  7875 20528\n","  6593 12151 17834  2094  2592  1999 20906  3906]...\n","Token IDs after removing padding: [2516, 4083, 1996, 9531, 1997, 17834, 23391, 1999, 20906, 6981, 7875, 20528, 6593, 12151, 17834, 2094, 2592, 1999, 20906, 3906]...\n","Tokens: ['title', 'learning', 'the', 'scope', 'of', 'hedge', 'cues', 'in', 'biomedical', 'texts', '##ab', '##stra', '##ct', 'identifying', 'hedge', '##d', 'information', 'in', 'biomedical', 'literature']...\n","Decoded Text: title learning the scope of hedge cues in biomedical textsabstract identifying hedged information in...\n","\n","Decoding batch index 63\n","Original Token IDs: [ 4919  2057  2265  2008  2011 15476  3672  2075  3622  4651  3001  2007\n","  2892 17002  8787  9324  2838  1996  5816  7561]...\n","Token IDs after removing padding: [4919, 2057, 2265, 2008, 2011, 15476, 3672, 2075, 3622, 4651, 3001, 2007, 2892, 17002, 8787, 9324, 2838, 1996, 5816, 7561]...\n","Tokens: ['specifically', 'we', 'show', 'that', 'by', 'aug', '##ment', '##ing', 'direct', 'transfer', 'systems', 'with', 'cross', 'ling', '##ual', 'cluster', 'features', 'the', 'relative', 'error']...\n","Decoded Text: specifically we show that by augmenting direct transfer systems with cross lingual cluster features ...\n","\n","Decoding batch index 64\n","Original Token IDs: [ 2516 26351  8093 17175  2271  8026  8486  9276  2005  3698  5449  7875\n"," 20528  6593  3001  2241  2006 26351  8093 17175]...\n","Token IDs after removing padding: [2516, 26351, 8093, 17175, 2271, 8026, 8486, 9276, 2005, 3698, 5449, 7875, 20528, 6593, 3001, 2241, 2006, 26351, 8093, 17175]...\n","Tokens: ['title', 'sync', '##hr', '##ono', '##us', 'bin', '##ari', '##zation', 'for', 'machine', 'translation', '##ab', '##stra', '##ct', 'systems', 'based', 'on', 'sync', '##hr', '##ono']...\n","Decoded Text: title synchronous binarization for machine translationabstract systems based on synchronous grammars...\n","\n","Decoding batch index 65\n","Original Token IDs: [ 2516  8422  2050  1037  1037  1055  4800  3413  9033 18697  4563 25523\n","  5813  2291  2012  1996  9530  3363  2249  4207]...\n","Token IDs after removing padding: [2516, 8422, 2050, 1037, 1037, 1055, 4800, 3413, 9033, 18697, 4563, 25523, 5813, 2291, 2012, 1996, 9530, 3363, 2249, 4207]...\n","Tokens: ['title', 'stanford', '##a', 'a', 'a', 's', 'multi', 'pass', 'si', '##eve', 'core', '##ference', 'resolution', 'system', 'at', 'the', 'con', '##ll', '2011', 'shared']...\n","Decoded Text: title stanforda a a s multi pass sieve coreference resolution system at the conll 2011 shared taskab...\n","\n","Decoding batch index 66\n","Original Token IDs: [ 2291  2171 10373  3967  8285  4523  8805  2863  9503  9353  2866  8285\n","  4523  2078 15088  8805  2863  9503  9353  2866]...\n","Token IDs after removing padding: [2291, 2171, 10373, 3967, 8285, 4523, 8805, 2863, 9503, 9353, 2866, 8285, 4523, 2078, 15088, 8805, 2863, 9503, 9353, 2866]...\n","Tokens: ['system', 'name', 'email', 'contact', 'auto', '##ps', 'diana', '##ma', 'sussex', 'ac', 'uk', 'auto', '##ps', '##n', '##vs', 'diana', '##ma', 'sussex', 'ac', 'uk']...\n","Decoded Text: system name email contact autops dianama sussex ac uk autopsnvs dianama sussex ac uk clr04 aw kena c...\n","\n","Decoding batch index 67\n","Original Token IDs: [ 1996  3259 22963  1998 23347  1996  2731  2051  2734  1998  2836  4719\n","  2011  2256  6310  4553  2121  2007  2048  2060]...\n","Token IDs after removing padding: [1996, 3259, 22963, 1998, 23347, 1996, 2731, 2051, 2734, 1998, 2836, 4719, 2011, 2256, 6310, 4553, 2121, 2007, 2048, 2060]...\n","Tokens: ['the', 'paper', 'compares', 'and', 'contrasts', 'the', 'training', 'time', 'needed', 'and', 'performance', 'achieved', 'by', 'our', 'modified', 'learn', '##er', 'with', 'two', 'other']...\n","Decoded Text: the paper compares and contrasts the training time needed and performance achieved by our modified l...\n","\n","Decoding batch index 68\n","Original Token IDs: [ 2516 27455  6671  7730  1999  4563 25523  5813  7875 20528  6593  1037\n"," 16166  3737  1997  1037  4563 25523  5813  2291]...\n","Token IDs after removing padding: [2516, 27455, 6671, 7730, 1999, 4563, 25523, 5813, 7875, 20528, 6593, 1037, 16166, 3737, 1997, 1037, 4563, 25523, 5813, 2291]...\n","Tokens: ['title', 'enforcing', 'transit', '##ivity', 'in', 'core', '##ference', 'resolution', '##ab', '##stra', '##ct', 'a', 'desirable', 'quality', 'of', 'a', 'core', '##ference', 'resolution', 'system']...\n","Decoded Text: title enforcing transitivity in coreference resolutionabstract a desirable quality of a coreference ...\n","\n","Decoding batch index 69\n","Original Token IDs: [ 2057 20742  2236  3733  2000 16306 14679  2006 12441 12034  2241  7189\n"," 15672  1999  2394  2008  2572 20806  6525  2618]...\n","Token IDs after removing padding: [2057, 20742, 2236, 3733, 2000, 16306, 14679, 2006, 12441, 12034, 2241, 7189, 15672, 1999, 2394, 2008, 2572, 20806, 6525, 2618]...\n","Tokens: ['we', 'articulated', 'general', 'easy', 'to', 'enforce', 'constraints', 'on', 'binary', 'verb', 'based', 'relation', 'phrases', 'in', 'english', 'that', 'am', '##eli', '##ora', '##te']...\n","Decoded Text: we articulated general easy to enforce constraints on binary verb based relation phrases in english ...\n","\n","Decoding batch index 70\n","Original Token IDs: [ 2516  2822 11054  2019  8114 10629  9324  2075  9896  1998  2049  4646\n","  2000  3019  2653  6364  3471  7875 20528  6593]...\n","Token IDs after removing padding: [2516, 2822, 11054, 2019, 8114, 10629, 9324, 2075, 9896, 1998, 2049, 4646, 2000, 3019, 2653, 6364, 3471, 7875, 20528, 6593]...\n","Tokens: ['title', 'chinese', 'whispers', 'an', 'efficient', 'graph', 'cluster', '##ing', 'algorithm', 'and', 'its', 'application', 'to', 'natural', 'language', 'processing', 'problems', '##ab', '##stra', '##ct']...\n","Decoded Text: title chinese whispers an efficient graph clustering algorithm and its application to natural langua...\n","\n","Decoding batch index 71\n","Original Token IDs: [ 2516  8150  2944  6789  2005 15488  2696  5910  6494  6593  2057  6235\n","  1037  8150  2944  3921  2000 25357  1037  7778]...\n","Token IDs after removing padding: [2516, 8150, 2944, 6789, 2005, 15488, 2696, 5910, 6494, 6593, 2057, 6235, 1037, 8150, 2944, 3921, 2000, 25357, 1037, 7778]...\n","Tokens: ['title', 'mixture', 'model', 'adaptation', 'for', 'sm', '##ta', '##bs', '##tra', '##ct', 'we', 'describe', 'a', 'mixture', 'model', 'approach', 'to', 'adapting', 'a', 'statistical']...\n","Decoded Text: title mixture model adaptation for smtabstract we describe a mixture model approach to adapting a st...\n","\n","Decoding batch index 72\n","Original Token IDs: [ 2516  1996  8256  4853  7159  2622  7875 20528  6593  2003  1037 24978\n","  2546  3569  2622  1999 13931  2241 15078 16105]...\n","Token IDs after removing padding: [2516, 1996, 8256, 4853, 7159, 2622, 7875, 20528, 6593, 2003, 1037, 24978, 2546, 3569, 2622, 1999, 13931, 2241, 15078, 16105]...\n","Tokens: ['title', 'the', 'berkeley', 'frame', '##net', 'project', '##ab', '##stra', '##ct', 'is', 'a', 'ns', '##f', 'supported', 'project', 'in', 'corpus', 'based', 'computational', 'lexi']...\n","Decoded Text: title the berkeley framenet projectabstract is a nsf supported project in corpus based computational...\n","\n","Decoding batch index 73\n","Original Token IDs: [ 2023  2003  1037  2200 11341  1998 11434  2765  1999  2008  2013  1037\n","  2200 15743  3988 20167  2478  2053 16105  8663]...\n","Token IDs after removing padding: [2023, 2003, 1037, 2200, 11341, 1998, 11434, 2765, 1999, 2008, 2013, 1037, 2200, 15743, 3988, 20167, 2478, 2053, 16105, 8663]...\n","Tokens: ['this', 'is', 'a', 'very', 'surprising', 'and', 'encouraging', 'result', 'in', 'that', 'from', 'a', 'very', 'naive', 'initial', 'approximation', 'using', 'no', 'lexi', '##con']...\n","Decoded Text: this is a very surprising and encouraging result in that from a very naive initial approximation usi...\n","\n","Decoding batch index 74\n","Original Token IDs: [10061 10474  3640  3229  2000  2312  6702  1997  2951  1999  2613  2051\n","  2021  2003 12536  2135 20810 10654  4842  2075]...\n","Token IDs after removing padding: [10061, 10474, 3640, 3229, 2000, 2312, 6702, 1997, 2951, 1999, 2613, 2051, 2021, 2003, 12536, 2135, 20810, 10654, 4842, 2075]...\n","Tokens: ['abstract', 'twitter', 'provides', 'access', 'to', 'large', 'volumes', 'of', 'data', 'in', 'real', 'time', 'but', 'is', 'notorious', '##ly', 'noisy', 'ham', '##per', '##ing']...\n","Decoded Text: abstract twitter provides access to large volumes of data in real time but is notoriously noisy hamp...\n","\n","Decoding batch index 75\n","Original Token IDs: [ 2516  2488  2773 12139  2015  2007 13588  2009  2290  4275  7875 20528\n","  6593  2023  2147 28062 13588  2773 12139  4725]...\n","Token IDs after removing padding: [2516, 2488, 2773, 12139, 2015, 2007, 13588, 2009, 2290, 4275, 7875, 20528, 6593, 2023, 2147, 28062, 13588, 2773, 12139, 4725]...\n","Tokens: ['title', 'better', 'word', 'alignment', '##s', 'with', 'supervised', 'it', '##g', 'models', '##ab', '##stra', '##ct', 'this', 'work', 'investigates', 'supervised', 'word', 'alignment', 'methods']...\n","Decoded Text: title better word alignments with supervised itg modelsabstract this work investigates supervised wo...\n","\n","Decoding batch index 76\n","Original Token IDs: [ 2256  3463  2265  2008  3820  2241  2522  2731  2064  6022  5335  6415\n","  4726  2836  2005  2235  6534  2951 13462  2015]...\n","Token IDs after removing padding: [2256, 3463, 2265, 2008, 3820, 2241, 2522, 2731, 2064, 6022, 5335, 6415, 4726, 2836, 2005, 2235, 6534, 2951, 13462, 2015]...\n","Tokens: ['our', 'results', 'show', 'that', 'agreement', 'based', 'co', 'training', 'can', 'significantly', 'improve', 'tag', '##ging', 'performance', 'for', 'small', 'seed', 'data', '##set', '##s']...\n","Decoded Text: our results show that agreement based co training can significantly improve tagging performance for ...\n","\n","Decoding batch index 77\n","Original Token IDs: [ 2516  4372 13149  2075  1996  3716  4216  2109  1999  1037  4555 23077\n","  2112  1997  4613  6415  4590  7875 20528  6593]...\n","Token IDs after removing padding: [2516, 4372, 13149, 2075, 1996, 3716, 4216, 2109, 1999, 1037, 4555, 23077, 2112, 1997, 4613, 6415, 4590, 7875, 20528, 6593]...\n","Tokens: ['title', 'en', '##rich', '##ing', 'the', 'knowledge', 'sources', 'used', 'in', 'a', 'maximum', 'entropy', 'part', 'of', 'speech', 'tag', '##ger', '##ab', '##stra', '##ct']...\n","Decoded Text: title enriching the knowledge sources used in a maximum entropy part of speech taggerabstract this p...\n","\n","Decoding batch index 78\n","Original Token IDs: [ 2516  2478 16840  2000  7949 11968  7741 13792  2005  3375  3444  2241\n","  5337 22556  7875 20528  6593  7091  2057  2031]...\n","Token IDs after removing padding: [2516, 2478, 16840, 2000, 7949, 11968, 7741, 13792, 2005, 3375, 3444, 2241, 5337, 22556, 7875, 20528, 6593, 7091, 2057, 2031]...\n","Tokens: ['title', 'using', 'restriction', 'to', 'extend', 'par', '##sing', 'algorithms', 'for', 'complex', 'feature', 'based', 'formal', '##isms', '##ab', '##stra', '##ct', 'conclusion', 'we', 'have']...\n","Decoded Text: title using restriction to extend parsing algorithms for complex feature based formalismsabstract co...\n","\n","Decoding batch index 79\n","Original Token IDs: [ 1996  8035  2003  2580  2005  2224  1999  2613  2088  5097  2107  2008\n"," 15873  2791  1998  2836  3314  2377  2019  2590]...\n","Token IDs after removing padding: [1996, 8035, 2003, 2580, 2005, 2224, 1999, 2613, 2088, 5097, 2107, 2008, 15873, 2791, 1998, 2836, 3314, 2377, 2019, 2590]...\n","Tokens: ['the', 'grammar', 'is', 'created', 'for', 'use', 'in', 'real', 'world', 'applications', 'such', 'that', 'robust', '##ness', 'and', 'performance', 'issues', 'play', 'an', 'important']...\n","Decoded Text: the grammar is created for use in real world applications such that robustness and performance issue...\n","\n","Decoding batch index 80\n","Original Token IDs: [ 2516 13588  4275  2005  4563 25523  5813  7875 20528  6593  3151  4083\n","  2241  4563 25523  2128 25918  3686  2011  2731]...\n","Token IDs after removing padding: [2516, 13588, 4275, 2005, 4563, 25523, 5813, 7875, 20528, 6593, 3151, 4083, 2241, 4563, 25523, 2128, 25918, 3686, 2011, 2731]...\n","Tokens: ['title', 'supervised', 'models', 'for', 'core', '##ference', 'resolution', '##ab', '##stra', '##ct', 'traditional', 'learning', 'based', 'core', '##ference', 're', '##oper', '##ate', 'by', 'training']...\n","Decoded Text: title supervised models for coreference resolutionabstract traditional learning based coreference re...\n","\n","Decoding batch index 81\n","Original Token IDs: [ 2516  5860 20026  3981  6024  6013  3635  2075  2005  5884  6789  1999\n","  7778  3698  5449  7875 20528  6593  2057  6235]...\n","Token IDs after removing padding: [2516, 5860, 20026, 3981, 6024, 6013, 3635, 2075, 2005, 5884, 6789, 1999, 7778, 3698, 5449, 7875, 20528, 6593, 2057, 6235]...\n","Tokens: ['title', 'disc', '##rim', '##ina', '##tive', 'instance', 'weight', '##ing', 'for', 'domain', 'adaptation', 'in', 'statistical', 'machine', 'translation', '##ab', '##stra', '##ct', 'we', 'describe']...\n","Decoded Text: title discriminative instance weighting for domain adaptation in statistical machine translationabst...\n","\n","Decoding batch index 82\n","Original Token IDs: [ 2045  2024  2093  2863  5558  2099  4280  1997  4353  2389 14402  2033\n","  2050  2469  2015  2029  2064  2022 17253  2004]...\n","Token IDs after removing padding: [2045, 2024, 2093, 2863, 5558, 2099, 4280, 1997, 4353, 2389, 14402, 2033, 2050, 2469, 2015, 2029, 2064, 2022, 17253, 2004]...\n","Tokens: ['there', 'are', 'three', '##ma', '##jo', '##r', 'classes', 'of', 'distribution', '##al', 'similarity', 'me', '##a', 'sure', '##s', 'which', 'can', 'be', 'characterised', 'as']...\n","Decoded Text: there are threemajor classes of distributional similarity mea sures which can be characterised as 1 ...\n","\n","Decoding batch index 83\n","Original Token IDs: [ 2516  7367  4168 10175  2289  4708  5718 20392  8982  2098  2394  2035\n","  2616  4708  7875 20528  6593  2023  3259  7534]...\n","Token IDs after removing padding: [2516, 7367, 4168, 10175, 2289, 4708, 5718, 20392, 8982, 2098, 2394, 2035, 2616, 4708, 7875, 20528, 6593, 2023, 3259, 7534]...\n","Tokens: ['title', 'se', '##me', '##val', '2007', 'task', '07', 'coarse', 'grain', '##ed', 'english', 'all', 'words', 'task', '##ab', '##stra', '##ct', 'this', 'paper', 'presents']...\n","Decoded Text: title semeval 2007 task 07 coarse grained english all words taskabstract this paper presents the coa...\n","\n","Decoding batch index 84\n","Original Token IDs: [ 2516  6882  7561 10788  1999  1996  2887 26262  2394  5287  2951  7875\n"," 20528  6593  2023  3259  5577  1037  4118  1997]...\n","Token IDs after removing padding: [2516, 6882, 7561, 10788, 1999, 1996, 2887, 26262, 2394, 5287, 2951, 7875, 20528, 6593, 2023, 3259, 5577, 1037, 4118, 1997]...\n","Tokens: ['title', 'automatic', 'error', 'detection', 'in', 'the', 'japanese', 'learners', 'english', 'spoken', 'data', '##ab', '##stra', '##ct', 'this', 'paper', 'describes', 'a', 'method', 'of']...\n","Decoded Text: title automatic error detection in the japanese learners english spoken dataabstract this paper desc...\n","\n","Decoding batch index 85\n","Original Token IDs: [ 5514 21933  4667  2007  6377  2653  4275  7875 20528  6593  8114 21933\n","  4667  2038  2042  1037  8050  3291  1999  3698]...\n","Token IDs after removing padding: [5514, 21933, 4667, 2007, 6377, 2653, 4275, 7875, 20528, 6593, 8114, 21933, 4667, 2038, 2042, 1037, 8050, 3291, 1999, 3698]...\n","Tokens: ['faster', 'deco', '##ding', 'with', 'integrated', 'language', 'models', '##ab', '##stra', '##ct', 'efficient', 'deco', '##ding', 'has', 'been', 'a', 'fundamental', 'problem', 'in', 'machine']...\n","Decoded Text: faster decoding with integrated language modelsabstract efficient decoding has been a fundamental pr...\n","\n","Decoding batch index 86\n","Original Token IDs: [ 2057  2424  2008  2043  4117  2512  6739 26186  2031  1037  2152  2504\n","  1997  3820  2007  1996  4493  2751  3115 26186]...\n","Token IDs after removing padding: [2057, 2424, 2008, 2043, 4117, 2512, 6739, 26186, 2031, 1037, 2152, 2504, 1997, 3820, 2007, 1996, 4493, 2751, 3115, 26186]...\n","Tokens: ['we', 'find', 'that', 'when', 'combined', 'non', 'expert', 'judgments', 'have', 'a', 'high', 'level', 'of', 'agreement', 'with', 'the', 'existing', 'gold', 'standard', 'judgments']...\n","Decoded Text: we find that when combined non expert judgments have a high level of agreement with the existing gol...\n","\n","Decoding batch index 87\n","Original Token IDs: [ 2516  4725  2005  1996 24209 11475 27453  9312  1997 16105  9289  2523\n","  5761  7875 20528  6593  2023  3259  7534  4725]...\n","Token IDs after removing padding: [2516, 4725, 2005, 1996, 24209, 11475, 27453, 9312, 1997, 16105, 9289, 2523, 5761, 7875, 20528, 6593, 2023, 3259, 7534, 4725]...\n","Tokens: ['title', 'methods', 'for', 'the', 'qu', '##ali', '##tative', 'evaluation', 'of', 'lexi', '##cal', 'association', 'measures', '##ab', '##stra', '##ct', 'this', 'paper', 'presents', 'methods']...\n","Decoded Text: title methods for the qualitative evaluation of lexical association measuresabstract this paper pres...\n","\n","Decoding batch index 88\n","Original Token IDs: [ 1999  1037  4800  3413  4613  5038  6123  2073  2502  6444  2015  2024\n","  2109  2000  3443  2034  3413  2502  6444 17779]...\n","Token IDs after removing padding: [1999, 1037, 4800, 3413, 4613, 5038, 6123, 2073, 2502, 6444, 2015, 2024, 2109, 2000, 3443, 2034, 3413, 2502, 6444, 17779]...\n","Tokens: ['in', 'a', 'multi', 'pass', 'speech', 'recognition', 'context', 'where', 'big', '##ram', '##s', 'are', 'used', 'to', 'create', 'first', 'pass', 'big', '##ram', 'lattice']...\n","Decoded Text: in a multi pass speech recognition context where bigrams are used to create first pass bigram lattic...\n","\n","Decoding batch index 89\n","Original Token IDs: [ 2516  2006  5157  2592 14676  7875 20528  6593  2012  2556 25357  2019\n","  2592 14676  2291  2000  2047  7832  2003  2019]...\n","Token IDs after removing padding: [2516, 2006, 5157, 2592, 14676, 7875, 20528, 6593, 2012, 2556, 25357, 2019, 2592, 14676, 2291, 2000, 2047, 7832, 2003, 2019]...\n","Tokens: ['title', 'on', 'demand', 'information', 'extraction', '##ab', '##stra', '##ct', 'at', 'present', 'adapting', 'an', 'information', 'extraction', 'system', 'to', 'new', 'topics', 'is', 'an']...\n","Decoded Text: title on demand information extractionabstract at present adapting an information extraction system ...\n","\n","Decoding batch index 90\n","Original Token IDs: [ 2516 18077  2075  8906  1999  3019  2653  6364 11566 11968  8043  3736\n","  5910  6494  6593  2093  2110  1997  1996  2396]...\n","Token IDs after removing padding: [2516, 18077, 2075, 8906, 1999, 3019, 2653, 6364, 11566, 11968, 8043, 3736, 5910, 6494, 6593, 2093, 2110, 1997, 1996, 2396]...\n","Tokens: ['title', 'exploit', '##ing', 'diversity', 'in', 'natural', 'language', 'processing', 'combining', 'par', '##ser', '##sa', '##bs', '##tra', '##ct', 'three', 'state', 'of', 'the', 'art']...\n","Decoded Text: title exploiting diversity in natural language processing combining parsersabstract three state of t...\n","\n","Decoding batch index 91\n","Original Token IDs: [ 2516  7778  7655  2241  5449  7875 20528  6593  2057 16599  1037  2047\n","  7655  2241  5449  2944  1998 21933  4667  9896]...\n","Token IDs after removing padding: [2516, 7778, 7655, 2241, 5449, 7875, 20528, 6593, 2057, 16599, 1037, 2047, 7655, 2241, 5449, 2944, 1998, 21933, 4667, 9896]...\n","Tokens: ['title', 'statistical', 'phrase', 'based', 'translation', '##ab', '##stra', '##ct', 'we', 'propose', 'a', 'new', 'phrase', 'based', 'translation', 'model', 'and', 'deco', '##ding', 'algorithm']...\n","Decoded Text: title statistical phrase based translationabstract we propose a new phrase based translation model a...\n","\n","Calculating ROUGE scores for sample index 0\n","Generated Summary: \n","Reference Summary: title early results for named entity recognition with conditional random fields feature induction and web enhanced lexiconsabstract conclusion to perform named entity extraction on the news articles in the conll 2003 english shared task several families of features are used all time shifted by 2 1 0 1 2 a the word itself b 16 character level regular expressions mostly concerning capitalization and digit patterns such as a a aa aa aa a d where a a and d indicate the regular expressions a z a z and 0 9 c 8 lexicons entered by hand such as honorifics days and months d 15 lexicons obtained from specific web sites such as countries publicly traded companies surnames stopwords and universities e 25 lexicons obtained by weblisting including people names organizations ngos and nationalities f all the above tests with prefix firstmention from any previous duplicate of the current word if capitalized a small amount of hand filtering was performed on some of the weblisting lexicons a java implemented first order crf was trained for about 12 hours on a 1ghz pentium with a gaussian prior variance of 0 5 inducing 1000 or fewer features down to a gain threshold of 5 0 each round of 10 iterations of l bfgs candidate conjunctions are limited to the 1000 atomic and existing features with highest gain performance results for each of the entity classes can be found in figure 1 the model achieved an overall f1 of 84 04 on the english test set using 6423 features using a set of fixed conjunction patterns instead of feature induction results in f1 73 34 with about 1 million features trialand error tuning the fixed patterns would likely improve this accuracy gains are expected from experimentation with the induction parameters and improved weblisting\n","rouge1 F-measure: 0.0000\n","rouge2 F-measure: 0.0000\n","rougeL F-measure: 0.0000\n","ROUGE Scores:\n","rouge1: 0.0000\n","rouge2: 0.0000\n","rougeL: 0.0000\n"]}]}],"metadata":{"kernelspec":{"display_name":"pytorch_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}